{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNNPMPdNfWXw7VM/WAawdLT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/PyTorch/blob/main/PyTorch_cvlassification_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device agnostic code"
      ],
      "metadata": {
        "id": "lXxwniOR5J8P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFCJJULU4jgH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Setup **device agnostic code**\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "mIeXxE0A5OcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset with Scikit-Learn's make_moons()\n",
        "import sklearn\n",
        "from sklearn.datasets import make_moons\n",
        "# Make 1500 circles\n",
        "n_samples = 1000\n",
        "\n",
        "# Create circles\n",
        "X, y = make_moons(n_samples,\n",
        "                    noise = 0.1,# we'll increase the noise in this data set\n",
        "                    random_state=42)\n"
      ],
      "metadata": {
        "id": "rM-iAKOX5cYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn data into a DataFrame\n",
        "import pandas as pd\n",
        "moons = pd.DataFrame({'X1': X[:, 0],\n",
        "                        'X2': X[:, 1],\n",
        "                        'label': y})\n",
        "moons.head()"
      ],
      "metadata": {
        "id": "ZmJUne5I5nFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(x=X[:, 0],\n",
        "            y=X[:, 1],\n",
        "            c=y,\n",
        "            cmap=plt.cm.RdYlBu);"
      ],
      "metadata": {
        "id": "hsuG6qTN6AZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Normalize features\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "qLpuC7228-PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The data is in numpy arrays, we need to turn into pytorch tensors\n",
        "import torch\n",
        "X = torch.from_numpy(X).type(torch.float)\n",
        "y = torch.from_numpy(y).type(torch.float)"
      ],
      "metadata": {
        "id": "Tvs57FDU-XTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data randomly\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n"
      ],
      "metadata": {
        "id": "gjCls5Pf-ZOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model for binary classification\n",
        "class MoonModelV2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Input layer to the first hidden layer (2 input features, 64 hidden units)\n",
        "        self.layer_1 = nn.Linear(2, 64)\n",
        "        self.relu_1 = nn.ReLU()  # Apply ReLU activation for non-linearity\n",
        "\n",
        "        # Second hidden layer (64 hidden units)\n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.relu_2 = nn.ReLU()  # Apply ReLU activation for non-linearity\n",
        "\n",
        "        # Output layer (1 unit for binary classification logits)\n",
        "        self.layer_3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the layers with activations\n",
        "        x = self.relu_1(self.layer_1(x))\n",
        "        x = self.relu_2(self.layer_2(x))\n",
        "        return self.layer_3(x)  # Return raw logits for use with BCEWithLogitsLoss\n",
        "\n"
      ],
      "metadata": {
        "id": "v1xWiZNs_BQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model and move it to the appropriate device (GPU)\n",
        "model_1a = MoonModelV2().to(device)\n",
        "\n",
        "# Ensure training and testing data are also on the same device\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n"
      ],
      "metadata": {
        "id": "AaCllt_fBSJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to initialize weights for the linear layers\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        # Xavier initialization for weights (good for layers with ReLU activations)\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        # Set biases to zero\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "# Apply the weight initialization to all layers of the model\n",
        "model_1a.apply(initialize_weights)\n"
      ],
      "metadata": {
        "id": "W77ADFruBrBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Binary Cross-Entropy Loss with Logits\n",
        "# This loss function is designed for binary classification and expects raw logits\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Use the Adam optimizer with a learning rate of 0.01 for efficient training\n",
        "# Adam dynamically adjusts learning rates for each parameter\n",
        "optimizer = torch.optim.Adam(params=model_1a.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "UzsFqlOBB6a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set manual seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Define the number of epochs for training- this is an easy model so it doesn't require much computing power\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Set the model to training mode\n",
        "    model_1a.train()\n",
        "\n",
        "    # Perform a forward pass to calculate logits\n",
        "    y_logits = model_1a(X_train).squeeze()  # Squeeze to ensure dimensions match\n",
        "\n",
        "    # Calculate the loss using BCEWithLogitsLoss\n",
        "    loss = loss_fn(y_logits, y_train.squeeze())\n",
        "\n",
        "    # Zero gradients to prevent accumulation\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backpropagate the loss to compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update model weights using the optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    # Set the model to evaluation mode for testing\n",
        "    model_1a.eval()\n",
        "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
        "        # Forward pass for the test data\n",
        "        test_logits = model_1a(X_test).squeeze()  # Logits for test data\n",
        "\n",
        "        # Calculate test loss\n",
        "        test_loss = loss_fn(test_logits, y_test.squeeze())\n",
        "\n",
        "        # Convert logits to probabilities and round to binary predictions\n",
        "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "\n",
        "        # Calculate test accuracy\n",
        "        test_acc = (test_pred == y_test.squeeze()).float().mean().item() * 100\n",
        "\n",
        "    # Print results every 100 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {loss:.4f}, Test Loss = {test_loss:.4f}, Test Acc = {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "Tl_SRQtECDo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1a.eval()\n",
        "with torch.inference_mode():\n",
        "    y_preds = torch.round(torch.sigmoid(model_1a(X_test))).squeeze()\n",
        "y_preds[:10], y_test[:10]"
      ],
      "metadata": {
        "id": "o9GRRbF4DLzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. (Optional) Remove the existing (likely invalid) helper_functions.py\n",
        "# !rm helper_functions.py\n",
        "\n",
        "# 2. Use the *raw* GitHub URL\n",
        "url_to_download = \"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\"\n",
        "\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "    print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "    print(\"Downloading helper_functions.py\")\n",
        "    request = requests.get(url_to_download)\n",
        "    with open(\"helper_functions.py\", \"wb\") as f:\n",
        "        f.write(request.content)\n"
      ],
      "metadata": {
        "id": "qBoOemzhDrm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_predictions, plot_decision_boundary\n"
      ],
      "metadata": {
        "id": "X1fJXr_NDtuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot decision Boundaries\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_1a, X_train, y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_1a, X_test, y_test)"
      ],
      "metadata": {
        "id": "qEXgam_DCMJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "usXGu1mBDyXq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}