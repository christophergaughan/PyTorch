{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP1diVn/AbflBDYa3LXYejr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/PyTorch/blob/main/PyTorch_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Hyperparameter             | Binary Classification                                                                                              | Multiclass Classification                                                                                  |\n",
        "|----------------------------|-------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
        "| **Input layer shape (in_features)** | Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction) | Same as binary classification                                                                             |\n",
        "| **Hidden layer(s)**        | Problem specific, minimum = 1, maximum = unlimited                                                               | Same as binary classification                                                                             |\n",
        "| **Neurons per hidden layer** | Problem specific, generally 10 to 512                                                                            | Same as binary classification                                                                             |\n",
        "| **Output layer shape (out_features)** | 1 (one class or the other)                                                                                  | 1 per class (e.g. 3 for food, person, or dog photo)                                                       |\n",
        "| **Hidden layer activation** | Usually ReLU (rectified linear unit) but can be many others                                                      | Same as binary classification                                                                             |\n",
        "| **Output activation**      | Sigmoid (`torch.sigmoid` in PyTorch)                                                                             | Softmax (`torch.softmax` in PyTorch)                                                                      |\n",
        "| **Loss function**          | Binary crossentropy (`torch.nn.BCELoss` in PyTorch)                                                              | Cross entropy (`torch.nn.CrossEntropyLoss` in PyTorch)                                                    |\n",
        "| **Optimizer**              | SGD (stochastic gradient descent), Adam (see `torch.optim` for more options)                                    | Same as binary classification                                                                             |\n"
      ],
      "metadata": {
        "id": "uVKHBLYJLUx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification is a problem connecting to whether one thing is identified with another"
      ],
      "metadata": {
        "id": "U-7nftIzMrSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make classification data and get it ready\n",
        "\n",
        "- This is a dataset already made in scikitlearn"
      ],
      "metadata": {
        "id": "HT05ni0-NBQn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYoqfWpsI-lt"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "# Make 1000 circles\n",
        "n_samples = 1000\n",
        "\n",
        "# Create circles\n",
        "X, y = make_circles(n_samples,\n",
        "                    noise = 0.03,\n",
        "                    random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X), len(y)"
      ],
      "metadata": {
        "id": "kWrKs39mNtnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'First 5 samples of X:\\n {X[:5]}')\n",
        "print(f'First 5 samples of y:\\n {y[:5]}')"
      ],
      "metadata": {
        "id": "EVNk5PF3NzqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "SV_1vU-2OB_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clearly, we have a binary classification problem here as we have only 0's and 1's in the predictor column $(y)$"
      ],
      "metadata": {
        "id": "eyw30mjFOWbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a dataframe\n",
        "import pandas as pd\n",
        "circles = pd.DataFrame({'X1': X[:, 0],\n",
        "                        'X2': X[:, 1],\n",
        "                        'label': y})\n",
        "circles.head()"
      ],
      "metadata": {
        "id": "VMtSojY0OSy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize data\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x=X[:, 0],\n",
        "            y=X[:, 1],\n",
        "            c=y,\n",
        "            cmap=plt.cm.RdYlBu);"
      ],
      "metadata": {
        "id": "CoY2gV6LO0pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is a *toy dataset*: small enough to experiment with, but it gives us a platform to employ PyTorch code\n",
        "\n",
        "**Our goal: separate the blue dots from the red dots**"
      ],
      "metadata": {
        "id": "ch88W8SePoQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check input and output shapes\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "tSRUPJPaPO3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The data is in numpy arrays, we need to turn into pytorch tensors\n",
        "import torch\n",
        "X = torch.from_numpy(X).type(torch.float)\n",
        "y = torch.from_numpy(y).type(torch.float)"
      ],
      "metadata": {
        "id": "CVcr6WPlQc2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[:5], y[:5]"
      ],
      "metadata": {
        "id": "2hofD8v1QrIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Shape of X: {X.shape}')\n",
        "print(f'Shape of y: {y.shape}')"
      ],
      "metadata": {
        "id": "KodNZ4lCQvL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Values for one sample of X: {X[0]} with shape: {X[0].shape}')\n",
        "print(f'Values for one sample of y: {y[0]} with shape: {y[0].shape}')"
      ],
      "metadata": {
        "id": "_7d3YFuYQ9N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create train and test splits"
      ],
      "metadata": {
        "id": "XMGHfSv2SKM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "id": "Om660CplRMPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.dtype, y.dtype"
      ],
      "metadata": {
        "id": "-d7tVr_oSYlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data randomly\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n"
      ],
      "metadata": {
        "id": "x0EvBfaYSvPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "xGAgoCn0Ttyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model\n",
        "\n",
        "1. Device agnostoc code\n",
        "2. Construct a model by subclassing `nn.Module`\n",
        "3. loss function and optimizer\n",
        "4. Create training and test loop"
      ],
      "metadata": {
        "id": "A96B4o3oUP1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device\n"
      ],
      "metadata": {
        "id": "YakUu5hPTwYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Subclass `nn.Module`\n",
        "2. Create 2 `nn.Linear()` layers capable of handling the shapes in our data\n",
        "3. Define `forward()` method that outlines the forward pass\n",
        "4. Instantiate an instance of our model class and sen to target `device`"
      ],
      "metadata": {
        "id": "h28PnnBZVuZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclass nn.Module\n",
        "class CircleModelV0(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # # Create nn.Linear layers capable of handling the shapes of our data\n",
        "        # self.layer_1 = nn.Linear(in_features=2,\n",
        "        #                          out_features=5) # upscales to 5 features (hidden layers\n",
        "\n",
        "        # self.layer_2 = nn.Linear(in_features=5,\n",
        "        #                          out_features=1) # we're predicting a 0 or 1\n",
        "        self.two_linear = nn.Sequential(\n",
        "            nn.Linear(in_features=2,\n",
        "                      out_features=5),\n",
        "            nn.Linear(in_features=5,\n",
        "                      out_features=1)\n",
        "        )\n",
        "    # define the forward pass\n",
        "    def forward(self, x):\n",
        "        return self.two_linear(x)\n",
        "    #   return self.layer_2(self.layer_1(x)) # x-> layer_1 -> layer_2 -> output\n",
        "\n",
        "# Instantiate instance of model class and send to target device\n",
        "model_0 = CircleModelV0().to(device)\n",
        "model_0\n"
      ],
      "metadata": {
        "id": "kI0BAKhWVEfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note in the code above:\n",
        "\n",
        "The forward pass in the provided code may seem \"backwards\" because the sequence in which the operations are written in code starts with the last layer and progresses to the input layer, but this is simply a reflection of the computation flow in neural networks. Let's break it down:\n",
        "\n",
        "#### Understanding the forward Pass\n",
        "Order of Operations:\n",
        "\n",
        "* When you call self.layer_1(x), the input x is passed through layer_1. * This produces the intermediate output of the first layer.\n",
        "* The intermediate output is then passed to self.layer_2, which produces the final output.\n",
        "\n",
        "In functional terms:\n",
        "`x -> layer_1 -> layer_2 -> output\n",
        "`\n",
        "However, the Python code is written as:\n",
        "`return self.layer_2(self.layer_1(x))\n",
        "`\n",
        "\n",
        "This is standard practice in programming because you apply the innermost function (layer 1) first and then the outermost function (layer 2).\n",
        "\n",
        "#### Why It Feels \"Backwards\":\n",
        "\n",
        "* Neural network layers are typically thought of as a forward progression from input to output.\n",
        "* In the `forward` method, the \"nesting\" structure can feel reversed because you start with the input, apply transformations in order, but write it with the innermost function first.\n",
        "\n",
        "#### It's Just Function Composition:\n",
        "\n",
        "* The code uses function composition, where one function's output is the input to the next. This is conceptually similar to:\n",
        "`f(g(x))\n",
        "`\n"
      ],
      "metadata": {
        "id": "QXo2mjsHbzDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(model_0.parameters()).device"
      ],
      "metadata": {
        "id": "9b3YRciCYrJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's replicate the model above using nn.Sequential\n",
        "model_0 = nn.Sequential(\n",
        "    nn.Linear(in_features=2,\n",
        "              out_features=5),\n",
        "    nn.Linear(in_features=5,\n",
        "              out_features=1)).to(device)\n",
        "\n",
        "model_0\n"
      ],
      "metadata": {
        "id": "okPtwW4TY_wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "NKZMXPcmfAMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make preds *rmbr to use the inference mode\n",
        "with torch.inference_mode():\n",
        "    untrained_preds = model_0(X_test.to(device))\n",
        "print(f'Length of preds: {len(untrained_preds)}')\n",
        "print(f'Shape of preds: {untrained_preds.shape}')\n",
        "print(f'First 10 preds: {untrained_preds[:10]}')\n",
        "print(f'First 10 y_test: {y_test[:10]}')"
      ],
      "metadata": {
        "id": "CTSdV1G9hYRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[:10], y_test[:10]"
      ],
      "metadata": {
        "id": "3ioaiiElitSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set-up loss function and optimizer\n",
        "\n",
        "Which loss and optimizer should we use?\n",
        "\n",
        "- Depends on the problem\n",
        "    - regression: MAE, MSE\n",
        "    - Classification: binary cross entropy or categorical cross entropy\n",
        "\n",
        "# Optimizer and Loss Functions in PyTorch\n",
        "\n",
        "However, the same optimizer function can often be used across different problem spaces.\n",
        "\n",
        "For example, the stochastic gradient descent optimizer (SGD, `torch.optim.SGD()`) can be used for a range of problems, and the same applies to the Adam optimizer (`torch.optim.Adam()`).\n",
        "\n",
        "| **Loss Function/Optimizer**               | **Problem Type**                   | **PyTorch Code**                        |\n",
        "|-------------------------------------------|-------------------------------------|-----------------------------------------|\n",
        "| **Stochastic Gradient Descent (SGD)**     | Classification, regression, many others. | `torch.optim.SGD()`                     |\n",
        "| **Adam Optimizer**                         | Classification, regression, many others. | `torch.optim.Adam()`                    |\n",
        "| **Binary Cross Entropy Loss**             | Binary classification               | `torch.nn.BCELossWithLogits` or `torch.nn.BCELoss` |\n",
        "| **Cross Entropy Loss**                    | Multi-class classification          | `torch.nn.CrossEntropyLoss`             |\n",
        "| **Mean Absolute Error (MAE) or L1 Loss**  | Regression                          | `torch.nn.L1Loss`                       |\n",
        "| **Mean Squared Error (MSE) or L2 Loss**   | Regression                          | `torch.nn.MSELoss`                      |\n"
      ],
      "metadata": {
        "id": "mglQZykoTc1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss function\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr=0.1)\n",
        "\n"
      ],
      "metadata": {
        "id": "tgWsGe7PjXaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "g3gb0_JjaVoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy- out of 100 examples what percentage does our model get right?\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc"
      ],
      "metadata": {
        "id": "5k0jeebHaaNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model\n",
        "\n",
        "1. Forward pass\n",
        "2. Calculate the loss\n",
        "3. Optimizer zero grad\n",
        "4. Loss backward (backpropagation)\n",
        "5. Optimizer step (gradient descent)"
      ],
      "metadata": {
        "id": "zqOSAp-NbgWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aslo we are going to perform the folowing:\n",
        "\n",
        "`going from raw logits -> prediction probabilities -> prediction labels`\n",
        "\n",
        "Our raw outputs from our model are logits. Convert into prediction probabilities  by passing them to some kind of activation function (e.g. sigmoid for binary classification or softmax for multiclass classificsation)\n",
        "\n",
        "Then we convert our models prediction probabilities to **prediction labels** by either rounding them or taking `argmax()`"
      ],
      "metadata": {
        "id": "cvzMb_jrcNay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0"
      ],
      "metadata": {
        "id": "JqgIrDKWbCj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 5 outputs of the forward pass on the test data\n",
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "    y_logits = model_0(X_test.to(device))[:5]\n",
        "y_logits"
      ],
      "metadata": {
        "id": "KkKHrPogdcTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[:5]"
      ],
      "metadata": {
        "id": "ecRPonR4d3VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we are performing a binary classification- use sigmoid activation function\n",
        "y_probs = torch.sigmoid(y_logits)\n",
        "y_probs"
      ],
      "metadata": {
        "id": "kUyUw95Ye_ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our predicition probability values, we need to perform a range-style rounding on them:\n",
        "* `y_pred_probs` >= 0.5 y = 1 (class 1)\n",
        "* `y_pred+probs` < 0.5 y=0 (class 0)"
      ],
      "metadata": {
        "id": "nAkWDKorfi8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find predicition probabilities\n",
        "y_preds = torch.round(y_probs)\n",
        "\n",
        "# In full (logits->pred_probs->pred_labels)\n",
        "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n",
        "y_pred_labels\n",
        "\n",
        "# Check for equality\n",
        "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n",
        "\n",
        "# Get rid of extra dimension\n",
        "y_preds.squeeze()"
      ],
      "metadata": {
        "id": "qlcVJBJqfRVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[:5]"
      ],
      "metadata": {
        "id": "zV8KE3PLfZbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a training and test loop"
      ],
      "metadata": {
        "id": "J6Bx3QO4iJym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "omqwSLw-iHW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "SarnNGAdy6QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "can also use a cuda manual seed"
      ],
      "metadata": {
        "id": "n8h9GXMuzPVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "-AQHSxajy-Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "id": "qyheycIOzMGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remember we are using BCEWITHLOGITSLOSS\n",
        "MORE NUMERICALLY STABLE (as per docs)\n",
        "\n",
        "## PyTorch BCEWithLogitsLoss\n",
        "\n",
        "In **PyTorch**, `BCEWithLogitsLoss` combines a Sigmoid layer and the Binary Cross-Entropy (BCE) loss in one single class. Mathematically, for each scalar input $( x_i)$ (the **logit**) and corresponding label $( y_i \\in \\{0,1\\} )$, the loss for one sample is given by:\n",
        "\n",
        "$$[\n",
        "\\ell_i = -\\Bigl[y_i \\cdot \\log\\bigl(\\sigma(x_i)\\bigr) \\;+\\; \\bigl(1 - y_i\\bigr)\\cdot \\log\\bigl(1 - \\sigma(x_i)\\bigr)\\Bigr],\n",
        "]$$\n",
        "\n",
        "where $( \\sigma(x_i) )$ is the Sigmoid function:\n",
        "\n",
        "$$[\n",
        "\\sigma(x_i) = \\frac{1}{1 + e^{-x_i}}.\n",
        "]$$\n",
        "\n",
        "If we have a mini-batch of \\( N \\) samples, the **mean** (or **sum**, depending on the `reduction` parameter) of all individual losses $( \\ell_i )$ is typically taken as the final scalar loss value:\n",
        "\n",
        "$$[\n",
        "\\text{BCEWithLogitsLoss} = \\frac{1}{N} \\sum_{i=1}^{N} \\ell_i.\n",
        "]$$\n",
        "\n",
        "### Optional Weights\n",
        "\n",
        "- **Weight:** In PyTorch, you can assign a per-sample weight $( w_i )$ to handle unbalanced data. This modifies the loss term to:\n",
        "\n",
        "  $$[\n",
        "  \\ell_i = -\\, w_i\\, \\Bigl[y_i \\cdot \\log\\bigl(\\sigma(x_i)\\bigr) + (1 - y_i)\\cdot \\log\\bigl(1 - \\sigma(x_i)\\bigr)\\Bigr].\n",
        "  ]$$\n",
        "\n",
        "- **Positional Weight (`pos_weight`)**: This is an additional multiplier for the positive targets, useful when you have *many* more negatives than positives. It modifies the loss term for $( y_i=1 )$. Specifically,\n",
        "\n",
        "$$\n",
        "\\ell_i = -\\Bigl[\\mathrm{pos\\_weight} \\cdot y_i \\cdot \\log(\\sigma(x_i)) + (1 - y_i) \\cdot \\log\\bigl(1 - \\sigma(x_i)\\bigr)\\Bigr].\n",
        "$$\n",
        "\n",
        "\n",
        "By accepting raw logits $( x_i )$ (i.e., values **before** the Sigmoid), `BCEWithLogitsLoss` is more numerically stable than applying a Sigmoid followed by a separate `BCELoss`.\n"
      ],
      "metadata": {
        "id": "Ne56_8d01pnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set number of epochs = 100\n",
        "epochs = 100\n",
        "\n",
        "# Put split data to target device\n",
        "X_train, y_train, X_test, y_test = X_train.to(device), y_train.to(device), X_test.to(device), y_test.to(device)\n",
        "\n",
        "# Build our training and avaluation loop\n",
        "for eopch in range(epochs):\n",
        "    #training\n",
        "    model_0.train()\n",
        "\n",
        "    # Forward pass- remember that squeeze removes an extra 1-dimension from a tensor\n",
        "    y_logits = model_0(X_train).squeeze()\n",
        "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels\n",
        "\n",
        "    # Calculate loss/accuracy- rememeber function above\n",
        "    # loss = loss_fn(torch.sigmoid(y_logits), # nn.BCELoss expects prediction probabilities as input\n",
        "    #                y_train)\n",
        "    loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss expects raw logits as input\n",
        "                   y_train)  # Note the order of the arguments here\n",
        "    acc = accuracy_fn(y_true=y_train,\n",
        "                      y_pred=y_pred)\n",
        "\n",
        "\n",
        "    # remember our accuracy function\n",
        "    acc = accuracy_fn(y_true=y_train,\n",
        "                      y_pred=y_pred)\n",
        "\n",
        "    # Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Loss backward- backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Optimizer step (gradient descent)\n",
        "    optimizer.step()\n",
        "\n",
        "    # Testing\n",
        "    model_0.eval()\n",
        "    with torch.inference_mode():\n",
        "        #forward pass\n",
        "        test_logits = model_0(X_test).squeeze()\n",
        "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "        # Calculate test loss/accuracy\n",
        "        test_loss = loss_fn(test_logits,\n",
        "                            y_test)\n",
        "        test_acc = accuracy_fn(y_true=y_test,\n",
        "                               y_pred=test_pred)\n",
        "\n",
        "    # Print out whats happening\n",
        "    if eopch % 10 == 0:\n",
        "        print(f\"Epoch: {eopch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zUljnLzlzbKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Our results are akin to flipping a coin. Not ideal"
      ],
      "metadata": {
        "id": "Ir3zuXLf--ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "circles.label.value_counts()"
      ],
      "metadata": {
        "id": "EGJyIhfYzgk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is our model not learning?\n",
        "\n",
        "Let's visualize\n",
        "\n",
        "to do so, we'll import a function called `plot_decision_boundary`\n",
        "\n",
        "we note a very important website for our endevors in ML/DL: https://madewithml.com/\n",
        "\n",
        "Specifically a repo by `Goku Mohandas`:\n",
        "https://madewithml.com/courses/mlops/evaluation/\n",
        "\n",
        "here we will use `mrdbourke's` helper function for visualizing our results\n",
        "\n"
      ],
      "metadata": {
        "id": "xzmSx3Q0_Pyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. (Optional) Remove the existing (likely invalid) helper_functions.py\n",
        "# !rm helper_functions.py\n",
        "\n",
        "# 2. Use the *raw* GitHub URL\n",
        "url_to_download = \"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\"\n",
        "\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "    print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "    print(\"Downloading helper_functions.py\")\n",
        "    request = requests.get(url_to_download)\n",
        "    with open(\"helper_functions.py\", \"wb\") as f:\n",
        "        f.write(request.content)\n"
      ],
      "metadata": {
        "id": "au6u9LaiI_P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_predictions, plot_decision_boundary\n"
      ],
      "metadata": {
        "id": "V5HMvV4uH7j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the decision boundary of the model\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_0, X_train, y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_0, X_test, y_test)"
      ],
      "metadata": {
        "id": "DbeTxupyIJy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This shows us why our model is getting such poor results."
      ],
      "metadata": {
        "id": "qrpRYQLrKWdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improve our model\n",
        "* add more layers- give the model more chances to learn about the patterns in the data\n",
        "* Add more hidden units - go from 5 hidden units to 10 hidden units\n",
        "* fit for longer\n",
        "* Change activation function - we're using sigmoid at the moment (good for binary data)\n",
        "* Change the learning rate (warning vanishing/exploding gradients)\n",
        "* Change the loss function\n",
        "* Change the optimization function\n",
        "\n",
        "These options are all from our model's perspective b/c they relate to the form of our model, rather than the data\n",
        "\n",
        "Because these options are all values we can change within the model itself- they are called **hyperparameters**\n",
        "\n",
        "##### Below we will\n",
        "* Add more hidden units\n",
        "* Increase the number of layers 2 -> 3\n",
        "* Increase the number of epochs 100 -> 1000\n",
        "*Ideally, we would only change 1 at a time b/c we will not know which of these improved/degraded our model. We do this just to save time"
      ],
      "metadata": {
        "id": "TzlryuqQLQno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CircleModelV1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_1 = nn.Linear(in_features=2,\n",
        "                                 out_features=10)\n",
        "        self.layer_2 = nn.Linear(in_features=10,\n",
        "                                 out_features=10)\n",
        "        self.layer_3 = nn.Linear(in_features=10,\n",
        "                                 out_features=1) # out just has 1 layer as it is binary choice\n",
        "    def forward(self, x):\n",
        "        # z = self.layer_1(x)\n",
        "        # z = self.layer_2(z)\n",
        "        # z = self.layer_3(z)\n",
        "        return self.layer_3(self.layer_2(self.layer_1(x))) # think f(g(x)) speeds up everything behind the scenes\n",
        "\n",
        "model_1 = CircleModelV1().to(device)\n",
        "model_1\n",
        "\n"
      ],
      "metadata": {
        "id": "3TNNVugFKOxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the loss function\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Create an optimizer\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(),\n",
        "                            lr=0.1)"
      ],
      "metadata": {
        "id": "UDH1VNFxQ2rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a training and evaluation loop\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "epochs = 1000 #training for longer\n",
        "\n",
        "# Put data on target device\n",
        "X_train, y_train, X_test, y_test = X_train.to(device), y_train.to(device), X_test.to(device), y_test.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model_1.train()\n",
        "    # forward pass\n",
        "    y_logits = model_1(X_train).squeeze()\n",
        "    y_pred = torch.round(torch.sigmoid(y_logits))\n",
        "\n",
        "    # Calculate loss/accuracy\n",
        "    loss = loss_fn(y_logits,\n",
        "                   y_train)\n",
        "    acc = accuracy_fn(y_true=y_train,\n",
        "                      y_pred=y_pred)\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # Optimizer step (gradient descent)\n",
        "    optimizer.step()\n",
        "\n",
        "    #Testing\n",
        "    model_1.eval()\n",
        "    with torch.inference_mode():\n",
        "        #Frward pass\n",
        "        test_logits = model_1(X_test).squeeze()\n",
        "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "\n",
        "        # Calculate loss/accuracy\n",
        "        test_loss = loss_fn(test_logits,\n",
        "                            y_test)\n",
        "        test_acc = accuracy_fn(y_true=y_test,\n",
        "                               y_pred=test_pred)\n",
        "    # Print out whats happening\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "uQZmx2HlRMwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundary\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_1, X_train, y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_1, X_test, y_test)"
      ],
      "metadata": {
        "id": "YJ4ylX9VULoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nope- coin flip"
      ],
      "metadata": {
        "id": "aV7rwv6uUdp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One ay to trouble-shoot a larger problem is to test out a smaller problem"
      ],
      "metadata": {
        "id": "JX9X3TsWVZ_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create some data (same as previous notebook)\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.01\n",
        "X_regression = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y_regression = weight * X_regression + bias\n",
        "\n",
        "# Check the data\n",
        "print(X_regression[:5], y_regression[:5])"
      ],
      "metadata": {
        "id": "HpIiWc9dUbLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = int(0.8 * len(X_regression))\n",
        "X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\n",
        "X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n",
        "# Check lengths of each\n",
        "len(X_train_regression), len(X_test_regression), len(y_train_regression), len(y_test_regression)"
      ],
      "metadata": {
        "id": "YpRMP0iFWbfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(train_data=X_train_regression,\n",
        "                 train_labels = y_train_regression,\n",
        "                 test_data = X_test_regression,\n",
        "                 test_labels = y_test_regression);"
      ],
      "metadata": {
        "id": "xGldHOQsXxTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_regression.shape, y_train_regression.shape"
      ],
      "metadata": {
        "id": "aah_praVY8dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our present model is set up to recieve 2 x-features, here we are only feeding in one feature."
      ],
      "metadata": {
        "id": "8z_F5tsZ03Jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_regression[:10]"
      ],
      "metadata": {
        "id": "5AWnpc3VZmIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "only one value as shown above"
      ],
      "metadata": {
        "id": "XN2r_T521N7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust `model_1` to fit a straight line- use `nn.Sequential`"
      ],
      "metadata": {
        "id": "m8m4MJhtaLih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same architecture as model_1 (but using nn.Sequential())\n",
        "model_2 = nn.Sequential(\n",
        "    nn.Linear(in_features=1,\n",
        "              out_features=10),\n",
        "    nn.Linear(in_features=10,\n",
        "              out_features=10),\n",
        "    nn.Linear(in_features=10,\n",
        "              out_features=1)\n",
        ").to(device)\n",
        "model_2"
      ],
      "metadata": {
        "id": "lRnhh6RVZ30I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "loss_fn = nn.L1Loss() # MAE loss with regression data\n",
        "optimizer = torch.optim.SGD(model_2.parameters(),\n",
        "                            lr=0.01)"
      ],
      "metadata": {
        "id": "KC3R4VWPa_fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs = 1000\n",
        "\n",
        "# Put the data on the target device\n",
        "X_train_regression, y_train_regression, X_test_regression, y_test_regression = X_train_regression.to(device), y_train_regression.to(device), X_test_regression.to(device), y_test_regression.to(device)\n",
        "\n",
        "# Training\n",
        "for epoch in range(epochs):\n",
        "    y_pred = model_2(X_train_regression)\n",
        "    loss = loss_fn(y_pred, y_train_regression)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # Testing\n",
        "    model_2.eval()\n",
        "    with torch.inference_mode():\n",
        "        test_pred = model_2(X_test_regression)\n",
        "        test_loss = loss_fn(test_pred, y_test_regression)\n",
        "\n",
        "    # Print out what is happening- Note, there is no accuracy here because we\n",
        "    # are doing regression\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch} | Loss: {loss:.5f} | Test Loss: {test_loss:.5f}\")"
      ],
      "metadata": {
        "id": "4-rbOtt72R5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OK, we see that model_2 can indeed learn as we see such small losses, particularly after we lowered the hyperparameter lr=0.1. -> lr = 0.01. This would lead us to think that model_2 is a valid approach**"
      ],
      "metadata": {
        "id": "H6B5aK2t5ELq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn on evaluation mode\n",
        "model_2.eval()\n",
        "\n",
        "# Create predictions (inference) with the model\n",
        "with torch.inference_mode():\n",
        "    y_preds = model_2(X_test_regression)\n",
        "\n",
        "# Plot data and predictions\n",
        "plot_predictions(train_data=X_train_regression,\n",
        "                 train_labels=y_train_regression,\n",
        "                 test_data=X_test_regression,\n",
        "                 test_labels=y_test_regression,\n",
        "                 predictions=y_preds)"
      ],
      "metadata": {
        "id": "FNUx1-aR4F0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wait, why is the the plot blank. We get the error `can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.`\n",
        "\n",
        "*explanation*: matplot lib references NumPy- which uses the cpu rather than the gpu!\n"
      ],
      "metadata": {
        "id": "N6TC-jnc7zSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write the correct code!\n",
        "model_2.eval()\n",
        "\n",
        "# Create predictions (inference) with the model\n",
        "with torch.inference_mode():\n",
        "    y_preds = model_2(X_test_regression)\n",
        "\n",
        "# Plot data and predictions\n",
        "plot_predictions(train_data=X_train_regression.cpu(),\n",
        "                 train_labels=y_train_regression.cpu(),\n",
        "                 test_data=X_test_regression.cpu(),\n",
        "                 test_labels=y_test_regression.cpu(),\n",
        "                 predictions=y_preds.cpu())"
      ],
      "metadata": {
        "id": "MJxH4QAI7sdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Now the question becomes: is it the data our model can't learn on? Is it the circular nature of the data? Our model only comprises linear functions- which are all related to straight lines- is it the fact that our data has some non-linear characteristics?*\n",
        "\n",
        "The big reveal here is that we will have to employ non linear activation functions!\n",
        "\n",
        "Neural networks have the benifit of combining straight lines with non-linear lines. Thus, our model was hamstrung by giving it the power to only use straight lines in its calculations. Our data is is non-linear, thus the introduction of non-linear components will be the key to our success."
      ],
      "metadata": {
        "id": "rRolwcTo9H0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recreating non-linear data (red and blue circles)"
      ],
      "metadata": {
        "id": "R-0enaaEAe2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make and plot data\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "n_samples = 1000\n",
        "X, y = make_circles(n_samples,\n",
        "                    noise=0.03,\n",
        "                    random_state=42)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)\n",
        "\n"
      ],
      "metadata": {
        "id": "m3ucv8V9857Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to tensors and then to train and test splits\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Turn data into tensors\n",
        "X = torch.from_numpy(X).type(torch.float)\n",
        "y = torch.from_numpy(y).type(torch.float)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# Check the data\n",
        "X_train[:5], y_train[:5]"
      ],
      "metadata": {
        "id": "LJutR0agBdOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a model with non-linearity\n",
        "\n",
        "### Non-Linear Activation Functions in PyTorch\n",
        "\n",
        "Activation functions introduce non-linearity into a neural network, allowing it to learn complex patterns in data. Below are some commonly used non-linear activation functions in PyTorch:\n",
        "\n",
        "1. **ReLU (Rectified Linear Unit)**:\n",
        "   - Formula: $( f(x) = \\max(0, x))$\n",
        "   - Introduces sparsity and avoids the vanishing gradient problem.\n",
        "   - Commonly used in hidden layers.\n",
        "\n",
        "2. **Sigmoid**:\n",
        "   - Formula: $( f(x) = \\frac{1}{1 + e^{-x}} )$\n",
        "   - Squashes input to a range between 0 and 1.\n",
        "   - Suitable for binary classification problems but may suffer from the vanishing gradient issue.\n",
        "\n",
        "3. **Tanh (Hyperbolic Tangent)**:\n",
        "   - Formula: $( f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} )$\n",
        "   - Squashes input to a range between -1 and 1.\n",
        "   - Zero-centered, which can help during optimization compared to Sigmoid.\n",
        "\n",
        "4. **Leaky ReLU**:\n",
        "   - Formula: $( f(x) = x ) if ( x > 0 ), else ( f(x) = \\alpha x )$ where $( \\alpha )$ is a small positive constant.\n",
        "   - Addresses the \"dying ReLU\" problem by allowing a small gradient for negative inputs.\n",
        "\n",
        "5. **Softmax**:\n",
        "   - Formula: $( f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} )$\n",
        "   - Converts outputs into probabilities, typically used in the final layer for multi-class classification.\n",
        "\n",
        "6. **ELU (Exponential Linear Unit)**:\n",
        "   - Formula:\n",
        "$$\n",
        "f(x) =\n",
        "\\begin{cases}\n",
        "x & \\text{if } x > 0 \\\\\n",
        "\\alpha (e^x - 1) & \\text{if } x \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "   - Similar to ReLU but smoothens the transition for negative inputs.\n",
        "\n",
        "7. **Swish**:\n",
        "   - Formula: $( f(x) = x \\cdot \\text{sigmoid}(x) )$\n",
        "   - A smooth, self-gated activation function known to improve performance on some deep learning tasks.\n",
        "\n",
        "#### Key Considerations:\n",
        "- **Choice of Activation Function**: Depends on the task and architecture. ReLU and its variants are widely used in hidden layers, while Softmax and Sigmoid are popular in output layers for classification tasks.\n",
        "- **Non-Linearity**: Activation functions enable neural networks to approximate complex, non-linear mappings.\n",
        "\n",
        "We could experiment with these activation functions to see their impact on the **make_circles** dataset, which is inherently non-linear!\n"
      ],
      "metadata": {
        "id": "y1LRFqYyFEMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class CircleModeV2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
        "        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
        "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
        "        self.relu = nn.ReLU() # This function is applied elelment-wise and is a non-linear act func\n",
        "    def forward(self, x):\n",
        "        # Where should we put our non-linear activation functions\n",
        "        return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x))))) # Remember chain rule\n",
        "\n",
        "model_3 = CircleModeV2().to(device)\n",
        "model_3"
      ],
      "metadata": {
        "id": "voIMGEl5CqL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why I think of the Chain Rule in Calculus\n",
        "\n",
        "the code snippet on the line 11 has a form that had confused me at first. One thing to remember is that it is conceptually similar to the **chain rule** in calculus! Here's why:\n",
        "\n",
        "---\n",
        "\n",
        "### The Chain Rule in Calculus\n",
        "The chain rule is used to compute the derivative of a composition of functions. If $( f(x) = g(h(x)) )$, then the derivative $( f'(x) )$ is:\n",
        "\n",
        "$$[\n",
        "f'(x) = g'(h(x)) \\cdot h'(x)\n",
        "]$$\n",
        "\n",
        "It works by applying the derivative of the outer function $( g )$, evaluated at the inner function $( h(x) )$, and multiplying it by the derivative of the inner function $( h )$.\n",
        "\n",
        "---\n",
        "\n",
        "### Thus when I see the code snippet below I think of that chain rule\n",
        "The code:\n",
        "```python\n",
        "return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n"
      ],
      "metadata": {
        "id": "Zzex6QgjLfZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the loss function\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Create an optimizer\n",
        "optimizer = torch.optim.SGD(params=model_3.parameters(),\n",
        "                            lr=0.01) # this hyperparameter will affect how long it takes for the model to complete"
      ],
      "metadata": {
        "id": "Jt0j8ukqL27I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a training and evaluation loop\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "epochs = 1000\n",
        "\n",
        "# Put data on target device\n",
        "X_train, y_train, X_test, y_test = X_train.to(device), y_train.to(device), X_test.to(device), y_test.to(device)\n",
        "\n",
        "# Loop tthroughh the data\n",
        "for epoch in range(epochs):\n",
        "    #Training\n",
        "    model_3.train()\n",
        "    # forward pass\n",
        "    y_logits = model_3(X_train).squeeze()\n",
        "    y_pred = torch.round(torch.sigmoid(y_logits))\n",
        "\n",
        "    # Calculate loss/accuracy\n",
        "    loss = loss_fn(y_logits,\n",
        "                   y_train)\n",
        "    acc = accuracy_fn(y_true=y_train,\n",
        "                      y_pred=y_pred)\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # Optimizer step (gradient descent)\n",
        "    optimizer.step()\n",
        "\n",
        "    #Testing\n",
        "    model_3.eval()\n",
        "    with torch.inference_mode():\n",
        "        #Frward pass\n",
        "        test_logits = model_3(X_test).squeeze()\n",
        "        test_pred = torch.round(torch.sigmoid(test_logits)) #logits -> prediction probabilities -> prediction labels\n",
        "\n",
        "        # Calculate loss/accuracy\n",
        "        test_loss = loss_fn(test_logits,\n",
        "                            y_test)\n",
        "        test_acc = accuracy_fn(y_true=y_test,\n",
        "                               y_pred=test_pred)\n",
        "    # Print out whats happening\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch} | Loss: {loss:.4f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "cV--5FZPNwgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the model\n",
        "* non linear activation function\n",
        "* lr = 0.01 (my change)"
      ],
      "metadata": {
        "id": "RTFcGQUldtJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.eval()\n",
        "with torch.inference_mode():\n",
        "    y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()\n",
        "y_preds[:10], y_test[:10]"
      ],
      "metadata": {
        "id": "L_ugxzV9b_2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot decision Boundaries\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_3, X_train, y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_3, X_test, y_test)"
      ],
      "metadata": {
        "id": "2Igj419CfczD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Still room for improvement"
      ],
      "metadata": {
        "id": "nFq-aBETgN2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replicate non-linear activation functions\n",
        "\n",
        "Neural networks, rather than us telling the model what to learn, we give it the tools to discover patterns in the data and it tries to figure out those patterns on its own\n",
        "\n",
        "These tools are linear and non-linear functions."
      ],
      "metadata": {
        "id": "S3MUBCMegwVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor\n",
        "A = torch.arange(-10, 10, 1, dtype = torch.float32)\n",
        "A.dtype\n"
      ],
      "metadata": {
        "id": "L0-V2kHyfxm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the tensor\n",
        "plt.plot(A);"
      ],
      "metadata": {
        "id": "MUfxmG05h3F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.relu(A));"
      ],
      "metadata": {
        "id": "u8R5LxqpiKOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x: torch.tensor) -> torch.Tensor:\n",
        "    return torch.maximum(torch.tensor(0), x) # inputs must be tensors\n",
        "relu(A)"
      ],
      "metadata": {
        "id": "it_GmHf4ih-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(relu(A));"
      ],
      "metadata": {
        "id": "KUiUkb_WjWg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets do the same for sigmoid\n",
        "def sigmoid(x: torch.tensor) -> torch.Tensor:\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "sigmoid(A)"
      ],
      "metadata": {
        "id": "lxLlIYHUjnMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(sigmoid(A));"
      ],
      "metadata": {
        "id": "B_ulMWYqj84R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SGpCe2T4kBUz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}