{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMWO0iuNNHStcNUqoWplQhB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/PyTorch/blob/main/PyTorch_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Hyperparameter             | Binary Classification                                                                                              | Multiclass Classification                                                                                  |\n",
        "|----------------------------|-------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
        "| **Input layer shape (in_features)** | Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction) | Same as binary classification                                                                             |\n",
        "| **Hidden layer(s)**        | Problem specific, minimum = 1, maximum = unlimited                                                               | Same as binary classification                                                                             |\n",
        "| **Neurons per hidden layer** | Problem specific, generally 10 to 512                                                                            | Same as binary classification                                                                             |\n",
        "| **Output layer shape (out_features)** | 1 (one class or the other)                                                                                  | 1 per class (e.g. 3 for food, person, or dog photo)                                                       |\n",
        "| **Hidden layer activation** | Usually ReLU (rectified linear unit) but can be many others                                                      | Same as binary classification                                                                             |\n",
        "| **Output activation**      | Sigmoid (`torch.sigmoid` in PyTorch)                                                                             | Softmax (`torch.softmax` in PyTorch)                                                                      |\n",
        "| **Loss function**          | Binary crossentropy (`torch.nn.BCELoss` in PyTorch)                                                              | Cross entropy (`torch.nn.CrossEntropyLoss` in PyTorch)                                                    |\n",
        "| **Optimizer**              | SGD (stochastic gradient descent), Adam (see `torch.optim` for more options)                                    | Same as binary classification                                                                             |\n"
      ],
      "metadata": {
        "id": "uVKHBLYJLUx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification is a problem connecting to whether one thing is identified with another"
      ],
      "metadata": {
        "id": "U-7nftIzMrSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make classification data and get it ready\n",
        "\n",
        "- This is a dataset already made in scikitlearn"
      ],
      "metadata": {
        "id": "HT05ni0-NBQn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYoqfWpsI-lt"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "# Make 1000 circles\n",
        "n_samples = 1000\n",
        "\n",
        "# Create circles\n",
        "X, y = make_circles(n_samples,\n",
        "                    noise = 0.03,\n",
        "                    random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X), len(y)"
      ],
      "metadata": {
        "id": "kWrKs39mNtnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'First 5 samples of X:\\n {X[:5]}')\n",
        "print(f'First 5 samples of y:\\n {y[:5]}')"
      ],
      "metadata": {
        "id": "EVNk5PF3NzqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "SV_1vU-2OB_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clearly, we have a binary classification problem here as we have only 0's and 1's in the predictor column $(y)$"
      ],
      "metadata": {
        "id": "eyw30mjFOWbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a dataframe\n",
        "import pandas as pd\n",
        "circles = pd.DataFrame({'X1': X[:, 0],\n",
        "                        'X2': X[:, 1],\n",
        "                        'label': y})\n",
        "circles.head()"
      ],
      "metadata": {
        "id": "VMtSojY0OSy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize data\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x=X[:, 0],\n",
        "            y=X[:, 1],\n",
        "            c=y,\n",
        "            cmap=plt.cm.RdYlBu);"
      ],
      "metadata": {
        "id": "CoY2gV6LO0pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is a *toy dataset*: small enough to experiment with, but it gives us a platform to employ PyTorch code\n",
        "\n",
        "**Our goal: separate the blue dots from the red dots**"
      ],
      "metadata": {
        "id": "ch88W8SePoQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check input and output shapes\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "tSRUPJPaPO3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The data is in numpy arrays, we need to turn into pytorch tensors\n",
        "import torch\n",
        "X = torch.from_numpy(X).type(torch.float)\n",
        "y = torch.from_numpy(y).type(torch.float)"
      ],
      "metadata": {
        "id": "CVcr6WPlQc2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[:5], y[:5]"
      ],
      "metadata": {
        "id": "2hofD8v1QrIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Shape of X: {X.shape}')\n",
        "print(f'Shape of y: {y.shape}')"
      ],
      "metadata": {
        "id": "KodNZ4lCQvL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Values for one sample of X: {X[0]} with shape: {X[0].shape}')\n",
        "print(f'Values for one sample of y: {y[0]} with shape: {y[0].shape}')"
      ],
      "metadata": {
        "id": "_7d3YFuYQ9N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create train and test splits"
      ],
      "metadata": {
        "id": "XMGHfSv2SKM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "id": "Om660CplRMPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.dtype, y.dtype"
      ],
      "metadata": {
        "id": "-d7tVr_oSYlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data randomly\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n"
      ],
      "metadata": {
        "id": "x0EvBfaYSvPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "xGAgoCn0Ttyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model\n",
        "\n",
        "1. Device agnostoc code\n",
        "2. Construct a model by subclassing `nn.Module`\n",
        "3. loss function and optimizer\n",
        "4. Create training and test loop"
      ],
      "metadata": {
        "id": "A96B4o3oUP1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device\n"
      ],
      "metadata": {
        "id": "YakUu5hPTwYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Subclass `nn.Module`\n",
        "2. Create 2 `nn.Linear()` layers capable of handling the shapes in our data\n",
        "3. Define `forward()` method that outlines the forward pass\n",
        "4. Instantiate an instance of our model class and sen to target `device`"
      ],
      "metadata": {
        "id": "h28PnnBZVuZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclass nn.Module\n",
        "class CircleModelV0(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # # Create nn.Linear layers capable of handling the shapes of our data\n",
        "        # self.layer_1 = nn.Linear(in_features=2,\n",
        "        #                          out_features=5) # upscales to 5 features (hidden layers\n",
        "\n",
        "        # self.layer_2 = nn.Linear(in_features=5,\n",
        "        #                          out_features=1) # we're predicting a 0 or 1\n",
        "        self.two_linear = nn.Sequential(\n",
        "            nn.Linear(in_features=2,\n",
        "                      out_features=5),\n",
        "            nn.Linear(in_features=5,\n",
        "                      out_features=1)\n",
        "        )\n",
        "    # define the forward pass\n",
        "    def forward(self, x):\n",
        "        return self.two_linear(x)\n",
        "    #   return self.layer_2(self.layer_1(x)) # x-> layer_1 -> layer_2 -> output\n",
        "\n",
        "# Instantiate instance of model class and send to target device\n",
        "model_0 = CircleModelV0().to(device)\n",
        "model_0\n"
      ],
      "metadata": {
        "id": "kI0BAKhWVEfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note in the code above:\n",
        "\n",
        "The forward pass in the provided code may seem \"backwards\" because the sequence in which the operations are written in code starts with the last layer and progresses to the input layer, but this is simply a reflection of the computation flow in neural networks. Let's break it down:\n",
        "\n",
        "#### Understanding the forward Pass\n",
        "Order of Operations:\n",
        "\n",
        "* When you call self.layer_1(x), the input x is passed through layer_1. * This produces the intermediate output of the first layer.\n",
        "* The intermediate output is then passed to self.layer_2, which produces the final output.\n",
        "\n",
        "In functional terms:\n",
        "`x -> layer_1 -> layer_2 -> output\n",
        "`\n",
        "However, the Python code is written as:\n",
        "`return self.layer_2(self.layer_1(x))\n",
        "`\n",
        "\n",
        "This is standard practice in programming because you apply the innermost function (layer 1) first and then the outermost function (layer 2).\n",
        "\n",
        "#### Why It Feels \"Backwards\":\n",
        "\n",
        "* Neural network layers are typically thought of as a forward progression from input to output.\n",
        "* In the `forward` method, the \"nesting\" structure can feel reversed because you start with the input, apply transformations in order, but write it with the innermost function first.\n",
        "\n",
        "#### It's Just Function Composition:\n",
        "\n",
        "* The code uses function composition, where one function's output is the input to the next. This is conceptually similar to:\n",
        "`f(g(x))\n",
        "`\n"
      ],
      "metadata": {
        "id": "QXo2mjsHbzDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(model_0.parameters()).device"
      ],
      "metadata": {
        "id": "9b3YRciCYrJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's replicate the model above using nn.Sequential\n",
        "model_0 = nn.Sequential(\n",
        "    nn.Linear(in_features=2,\n",
        "              out_features=5),\n",
        "    nn.Linear(in_features=5,\n",
        "              out_features=1)).to(device)\n",
        "\n",
        "model_0\n"
      ],
      "metadata": {
        "id": "okPtwW4TY_wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "NKZMXPcmfAMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make preds *rmbr to use the inference mode\n",
        "with torch.inference_mode():\n",
        "    untrained_preds = model_0(X_test.to(device))\n",
        "print(f'Length of preds: {len(untrained_preds)}')\n",
        "print(f'Shape of preds: {untrained_preds.shape}')\n",
        "print(f'First 10 preds: {untrained_preds[:10]}')\n",
        "print(f'First 10 y_test: {y_test[:10]}')"
      ],
      "metadata": {
        "id": "CTSdV1G9hYRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[:10], y_test[:10]"
      ],
      "metadata": {
        "id": "3ioaiiElitSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set-up loss function and optimizer\n",
        "\n",
        "Which loss and optimizer should we use?\n",
        "\n",
        "- Depends on the problem\n",
        "    - regression: MAE, MSE\n",
        "    - Classification: binary cross entropy or categorical cross entropy\n",
        "\n",
        "# Optimizer and Loss Functions in PyTorch\n",
        "\n",
        "However, the same optimizer function can often be used across different problem spaces.\n",
        "\n",
        "For example, the stochastic gradient descent optimizer (SGD, `torch.optim.SGD()`) can be used for a range of problems, and the same applies to the Adam optimizer (`torch.optim.Adam()`).\n",
        "\n",
        "| **Loss Function/Optimizer**               | **Problem Type**                   | **PyTorch Code**                        |\n",
        "|-------------------------------------------|-------------------------------------|-----------------------------------------|\n",
        "| **Stochastic Gradient Descent (SGD)**     | Classification, regression, many others. | `torch.optim.SGD()`                     |\n",
        "| **Adam Optimizer**                         | Classification, regression, many others. | `torch.optim.Adam()`                    |\n",
        "| **Binary Cross Entropy Loss**             | Binary classification               | `torch.nn.BCELossWithLogits` or `torch.nn.BCELoss` |\n",
        "| **Cross Entropy Loss**                    | Multi-class classification          | `torch.nn.CrossEntropyLoss`             |\n",
        "| **Mean Absolute Error (MAE) or L1 Loss**  | Regression                          | `torch.nn.L1Loss`                       |\n",
        "| **Mean Squared Error (MSE) or L2 Loss**   | Regression                          | `torch.nn.MSELoss`                      |\n"
      ],
      "metadata": {
        "id": "mglQZykoTc1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss function\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr=0.1)\n",
        "\n"
      ],
      "metadata": {
        "id": "tgWsGe7PjXaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "g3gb0_JjaVoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy- out of 100 examples what percentage does our model get right?\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc"
      ],
      "metadata": {
        "id": "5k0jeebHaaNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model\n",
        "\n",
        "1. Forward pass\n",
        "2. Calculate the loss\n",
        "3. Optimizer zero grad\n",
        "4. Loss backward (backpropagation)\n",
        "5. Optimizer step (gradient descent)"
      ],
      "metadata": {
        "id": "zqOSAp-NbgWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aslo we are going to perform the folowing:\n",
        "\n",
        "`going from raw logits -> prediction probabilities -> prediction labels`\n",
        "\n",
        "Our raw outputs from our model are logits. Convert into prediction probabilities  by passing them to some kind of activation function (e.g. sigmoid for binary classification or softmax for multiclass classificsation)\n",
        "\n",
        "Then we convert our models prediction probabilities to **prediction labels** by either rounding them or taking `argmax()`"
      ],
      "metadata": {
        "id": "cvzMb_jrcNay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0"
      ],
      "metadata": {
        "id": "JqgIrDKWbCj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 5 outputs of the forward pass on the test data\n",
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "    y_logits = model_0(X_test.to(device))[:5]\n",
        "y_logits"
      ],
      "metadata": {
        "id": "KkKHrPogdcTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[:5]"
      ],
      "metadata": {
        "id": "ecRPonR4d3VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we are performing a binary classification- use sigmoid activation function\n",
        "y_probs = torch.sigmoid(y_logits)\n",
        "y_probs"
      ],
      "metadata": {
        "id": "kUyUw95Ye_ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our predicition probability values, we need to perform a range-style rounding on them:\n",
        "* `y_pred_probs` >= 0.5 y = 1 (class 1)\n",
        "* `y_pred+probs` < 0.5 y=0 (class 0)"
      ],
      "metadata": {
        "id": "nAkWDKorfi8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find predicition probabilities\n",
        "y_preds = torch.round(y_probs)\n",
        "\n",
        "# In full (logits->pred_probs->pred_labels)\n",
        "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n",
        "y_pred_labels\n",
        "\n",
        "# Check for equality\n",
        "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n",
        "\n",
        "# Get rid of extra dimension\n",
        "y_preds.squeeze()"
      ],
      "metadata": {
        "id": "qlcVJBJqfRVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[:5]"
      ],
      "metadata": {
        "id": "zV8KE3PLfZbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a training and test loop"
      ],
      "metadata": {
        "id": "J6Bx3QO4iJym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "omqwSLw-iHW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "SarnNGAdy6QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "can also use a cuda manual seed"
      ],
      "metadata": {
        "id": "n8h9GXMuzPVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "-AQHSxajy-Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "id": "qyheycIOzMGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remember we are using BCEWITHLOGITSLOSS\n",
        "MORE NUMERICALLY STABLE (as per docs)\n",
        "\n",
        "## PyTorch BCEWithLogitsLoss\n",
        "\n",
        "In **PyTorch**, `BCEWithLogitsLoss` combines a Sigmoid layer and the Binary Cross-Entropy (BCE) loss in one single class. Mathematically, for each scalar input $( x_i)$ (the **logit**) and corresponding label $( y_i \\in \\{0,1\\} )$, the loss for one sample is given by:\n",
        "\n",
        "$$[\n",
        "\\ell_i = -\\Bigl[y_i \\cdot \\log\\bigl(\\sigma(x_i)\\bigr) \\;+\\; \\bigl(1 - y_i\\bigr)\\cdot \\log\\bigl(1 - \\sigma(x_i)\\bigr)\\Bigr],\n",
        "]$$\n",
        "\n",
        "where $( \\sigma(x_i) )$ is the Sigmoid function:\n",
        "\n",
        "$$[\n",
        "\\sigma(x_i) = \\frac{1}{1 + e^{-x_i}}.\n",
        "]$$\n",
        "\n",
        "If we have a mini-batch of \\( N \\) samples, the **mean** (or **sum**, depending on the `reduction` parameter) of all individual losses $( \\ell_i )$ is typically taken as the final scalar loss value:\n",
        "\n",
        "$$[\n",
        "\\text{BCEWithLogitsLoss} = \\frac{1}{N} \\sum_{i=1}^{N} \\ell_i.\n",
        "]$$\n",
        "\n",
        "### Optional Weights\n",
        "\n",
        "- **Weight:** In PyTorch, you can assign a per-sample weight $( w_i )$ to handle unbalanced data. This modifies the loss term to:\n",
        "\n",
        "  $$[\n",
        "  \\ell_i = -\\, w_i\\, \\Bigl[y_i \\cdot \\log\\bigl(\\sigma(x_i)\\bigr) + (1 - y_i)\\cdot \\log\\bigl(1 - \\sigma(x_i)\\bigr)\\Bigr].\n",
        "  ]$$\n",
        "\n",
        "- **Positional Weight (`pos_weight`)**: This is an additional multiplier for the positive targets, useful when you have *many* more negatives than positives. It modifies the loss term for $( y_i=1 )$. Specifically,\n",
        "\n",
        "  $$[\n",
        "  \\ell_i = -\\Bigl[\\text{pos\\_weight} \\cdot y_i \\cdot \\log(\\sigma(x_i)) + (1 - y_i) \\cdot \\log\\bigl(1 - \\sigma(x_i)\\bigr)\\Bigr].\n",
        "  ]$$\n",
        "\n",
        "By accepting raw logits $( x_i )$ (i.e., values **before** the Sigmoid), `BCEWithLogitsLoss` is more numerically stable than applying a Sigmoid followed by a separate `BCELoss`.\n"
      ],
      "metadata": {
        "id": "Ne56_8d01pnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set number of epochs = 100\n",
        "epochs = 100\n",
        "\n",
        "# Put split data to target device\n",
        "X_train, y_train, X_test, y_test = X_train.to(device), y_train.to(device), X_test.to(device), y_test.to(device)\n",
        "\n",
        "# Build our training and avaluation loop\n",
        "for eopch in range(epochs):\n",
        "    #training\n",
        "    model_0.train()\n",
        "\n",
        "    # Forward pass- remember that squeeze removes an extra 1-dimension from a tensor\n",
        "    y_logits = model_0(X_train).squeeze()\n",
        "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels\n",
        "\n",
        "    # Calculate loss/accuracy- rememeber function above\n",
        "    # loss = loss_fn(torch.sigmoid(y_logits), # nn.BCELoss expects prediction probabilities as input\n",
        "    #                y_train)\n",
        "    loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss expects raw logits as input\n",
        "                   y_train)  # Note the order of the arguments here\n",
        "    acc = accuracy_fn(y_true=y_train,\n",
        "                      y_pred=y_pred)\n",
        "\n",
        "\n",
        "    # remember our accuracy function\n",
        "    acc = accuracy_fn(y_true=y_train,\n",
        "                      y_pred=y_pred)\n",
        "\n",
        "    # Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Loss backward- backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Optimizer step (gradient descent)\n",
        "    optimizer.step()\n",
        "\n",
        "    # Testing\n",
        "    model_0.eval()\n",
        "    with torch.inference_mode():\n",
        "        #forward pass\n",
        "        test_logits = model_0(X_test).squeeze()\n",
        "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "        # Calculate test loss/accuracy\n",
        "        test_loss = loss_fn(test_logits,\n",
        "                            y_test)\n",
        "        test_acc = accuracy_fn(y_true=y_test,\n",
        "                               y_pred=test_pred)\n",
        "\n",
        "    # Print out whats happening\n",
        "    if eopch % 10 == 0:\n",
        "        print(f\"Epoch: {eopch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zUljnLzlzbKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Our results are akin to flipping a coin. Not ideal"
      ],
      "metadata": {
        "id": "Ir3zuXLf--ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "circles.label.value_counts()"
      ],
      "metadata": {
        "id": "EGJyIhfYzgk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is our model not learning?\n",
        "\n",
        "Let's visualize\n",
        "\n",
        "to do so, we'll import a function called `plot_decision_boundary`\n",
        "\n",
        "we note a very important website for our endevors in ML/DL: https://madewithml.com/\n",
        "\n",
        "Specifically a repo by `Goku Mohandas`:\n",
        "https://madewithml.com/courses/mlops/evaluation/\n",
        "\n",
        "here we will use `mrdbourke's` helper function for visualizing our results\n",
        "\n"
      ],
      "metadata": {
        "id": "xzmSx3Q0_Pyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. (Optional) Remove the existing (likely invalid) helper_functions.py\n",
        "!rm helper_functions.py\n",
        "\n",
        "# 2. Use the *raw* GitHub URL\n",
        "url_to_download = \"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\"\n",
        "\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "    print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "    print(\"Downloading helper_functions.py\")\n",
        "    request = requests.get(url_to_download)\n",
        "    with open(\"helper_functions.py\", \"wb\") as f:\n",
        "        f.write(request.content)\n"
      ],
      "metadata": {
        "id": "au6u9LaiI_P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_predictions, plot_decision_boundary\n"
      ],
      "metadata": {
        "id": "V5HMvV4uH7j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the decision boundary of the model\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_0, X_train, y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_0, X_test, y_test)"
      ],
      "metadata": {
        "id": "DbeTxupyIJy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This shows us why our model is getting such poor results."
      ],
      "metadata": {
        "id": "qrpRYQLrKWdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improve our model\n",
        "* add more layers- give the model more chances to learn about the patterns in the data\n",
        "* Add more hidden units - go from 5 hidden units to 10 hidden units\n",
        "* fit for longer\n",
        "* Change activation function - we're using sigmoid at the moment (good for binary data)\n",
        "* Change the learning rate (warning vanishing/exploding gradients)\n",
        "* Change the loss function\n",
        "* Change the optimization function\n",
        "\n",
        "These options are all from our model's perspective b/c they relate to the form of our model, rather than the data\n",
        "\n",
        "Because these options are all values we can change within the model itself- they are called **hyperparameters**\n",
        "\n",
        "##### Below we will\n",
        "* Add more hidden units\n",
        "* Increase the number of layers 2 -> 3\n",
        "* Increase the number of epochs 100 -> 1000\n",
        "*Ideally, we would only change 1 at a time b/c we will not know which of these improved/degraded our model. We do this just to save time"
      ],
      "metadata": {
        "id": "TzlryuqQLQno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CircleModelV1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_1 = nn.Linear(in_features=2,\n",
        "                                 out_features=10)\n",
        "        self.layer_2 = nn.Linear(in_features=10,\n",
        "                                 out_features=10)\n",
        "        self.layer_3 = nn.Linear(in_features=10,\n",
        "                                 out_features=1) # out just has 1 layer as it is binary choice\n",
        "    def forward(self, x):\n",
        "        # z = self.layer_1(x)\n",
        "        # z = self.layer_2(z)\n",
        "        # z = self.layer_3(z)\n",
        "        return self.layer_3(self.layer_2(self.layer_1(x))) # think f(g(x)) speeds up everything behind the scenes\n",
        "\n",
        "model_1 = CircleModelV1().to(device)\n",
        "model_1\n",
        "\n"
      ],
      "metadata": {
        "id": "3TNNVugFKOxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the loss function\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Create an optimizer\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(),\n",
        "                            lr=0.1)"
      ],
      "metadata": {
        "id": "UDH1VNFxQ2rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a training and evaluation loop\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "epochs = 1000 #training for longer\n",
        "\n",
        "# Put data on target device\n",
        "X_train, y_train, X_test, y_test = X_train.to(device), y_train.to(device), X_test.to(device), y_test.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model_1.train()\n",
        "    # forward pass\n",
        "    y_logits = model_1(X_train).squeeze()\n",
        "    y_pred = torch.round(torch.sigmoid(y_logits))\n",
        "\n",
        "    # Calculate loss/accuracy\n",
        "    loss = loss_fn(y_logits,\n",
        "                   y_train)\n",
        "    acc = accuracy_fn(y_true=y_train,\n",
        "                      y_pred=y_pred)\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # Optimizer step (gradient descent)\n",
        "    optimizer.step()\n",
        "\n",
        "    #Testing\n",
        "    model_1.eval()\n",
        "    with torch.inference_mode():\n",
        "        #Frward pass\n",
        "        test_logits = model_1(X_test).squeeze()\n",
        "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "\n",
        "        # Calculate loss/accuracy\n",
        "        test_loss = loss_fn(test_logits,\n",
        "                            y_test)\n",
        "        test_acc = accuracy_fn(y_true=y_test,\n",
        "                               y_pred=test_pred)\n",
        "    # Print out whats happening\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "uQZmx2HlRMwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundary\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_1, X_train, y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_1, X_test, y_test)"
      ],
      "metadata": {
        "id": "YJ4ylX9VULoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nope- coin flip"
      ],
      "metadata": {
        "id": "aV7rwv6uUdp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One ay to trouble-shoot a larger problem is to test out a smaller problem"
      ],
      "metadata": {
        "id": "JX9X3TsWVZ_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create some data (same as previous notebook)\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.01\n",
        "X_regression = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y_regression = weight * X_regression + bias\n",
        "\n",
        "# Check the data\n",
        "print(X_regression[:5], y_regression[:5])"
      ],
      "metadata": {
        "id": "HpIiWc9dUbLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = int(0.8 * len(X_regression))\n",
        "X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\n",
        "X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n",
        "# Check lengths of each\n",
        "len(X_train_regression), len(X_test_regression), len(y_train_regression), len(y_test_regression)"
      ],
      "metadata": {
        "id": "YpRMP0iFWbfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(train_data=X_train_regression,\n",
        "                 train_labels = y_train_regression,\n",
        "                 test_data = X_test_regression,\n",
        "                 test_labels = y_test_regression);"
      ],
      "metadata": {
        "id": "xGldHOQsXxTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_regression.shape, y_train_regression.shape"
      ],
      "metadata": {
        "id": "aah_praVY8dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_regression[:10], y_train_regression[:10]"
      ],
      "metadata": {
        "id": "5AWnpc3VZmIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust `model_1` to fit a straight line"
      ],
      "metadata": {
        "id": "m8m4MJhtaLih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same architecture as model_1 (but using nn.Sequential())\n",
        "model_2 = nn.Sequential(\n",
        "    nn.Linear(in_features=1,\n",
        "              out_features=10),\n",
        "    nn.Linear(in_features=10,\n",
        "              out_features=10),\n",
        "    nn.Linear(in_features=10,\n",
        "              out_features=1)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "lRnhh6RVZ30I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KC3R4VWPa_fT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}