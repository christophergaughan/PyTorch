{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "mount_file_id": "1W8Zp4YeTlkN4ZdW_QWdvkimMZR-yjMQy",
      "authorship_tag": "ABX9TyOO6vPNX4ew3Y+AXXXxYK9b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/PyTorch/blob/main/ComputerVision_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Vision- Using PYTorch\n",
        "\n",
        "**Basis**\n",
        "\n",
        "pixels are read as RGB colors and turned into --> numbers (tensors) or `numerical encoding` --> model (algorithm) --> output probability that the image is X ot Y or Z\n",
        "\n",
        "**Details**\n",
        " Tensors contain the following information:\n",
        " 1. Width of image\n",
        " 2. Height of image\n",
        " 3. Color channels == 3 (RGB)\n",
        " depending on what algorithm you're working with data as tensors whose ID is as follows:\n",
        "\n",
        " [batch_size, height, width, color_channels] OR [batch_size, color_channels, height, width]\n",
        "\n",
        " These will be mainly CNN models\n",
        "\n",
        " We will be working with `torch.nn.Conv2d`\n",
        "\n",
        " ## Computer version libraries in PyTorch\n",
        "\n",
        "* `torchvision`- base domain library for PyTorch computer vision-\n",
        "  https://pytorch.org/vision/stable/index.html\n",
        "* `torchvision.datassets`get datasets and loading functions here:\n",
        "  https://pytorch.org/vision/stable/datasets.html#built-in-datasets\n",
        "* `torchvision.models` get pre-trained computer vision models i.e. have pretrained weights, etc. that you can leverage for your own problems.\n",
        "* `torchvision.transforms`- functions for manipulating your vision data (images) to be suitable for use with an ML model.\n",
        "* `torch.utils.Dataset`- Base dataset class for PyTorch.\n",
        "* `torch.utils.data.DataLoader` - Creates a Python iterable over a dataset\n",
        "\n",
        "Torchvision supports common computer vision transformations in the torchvision.transforms and torchvision.transforms.v2 modules. Transforms can be used to transform or augment data for training or inference of different tasks (image classification, detection, segmentation, video classification).\n",
        "\n",
        "* PIL is the Python Imaging Library by Fredrik Lundh and contributors.\n",
        "\n",
        "### torchvision.datasets\n",
        "\n",
        "All datasets are subclasses of torch.utils.data.Dataset i.e, they have __getitem__ and __len__ methods implemented. Hence, they can all be passed to a torch.utils.data.DataLoader which can load multiple samples parallelly using torch.multiprocessing workers. For example:\n",
        "```\n",
        "imagenet_data = torchvision.datasets.ImageNet('path/to/imagenet_root/')\n",
        "data_loader = torch.utils.data.DataLoader(imagenet_data,\n",
        "                                          batch_size=4,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=args.nThreads)\n",
        "```"
      ],
      "metadata": {
        "id": "nPbopb9gp_qY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peQatlzGl9MS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting a dataset\n",
        "\n",
        "we will be using `fashion.mnist` datset- greyscale images of clothing\n",
        "basic dataset for implementation here\n",
        "\n",
        "Be aware that IMAGENET  is the gold standard for computer vision evaluations"
      ],
      "metadata": {
        "id": "vvnEhTotYExd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torchvision.datasets.FashionMNIST(root: str, train: bool = True, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) → None[source]`\n",
        "\n",
        "### Fashion-MNIST Dataset.\n",
        "\n",
        "Parameters:\n",
        "* **root (string)** – Root directory of dataset where FashionMNIST/processed/training.pt and FashionMNIST/processed/test.pt exist.\n",
        "* **train (bool, optional)** – If True, creates dataset from training.pt, otherwise from test.pt.\n",
        "* **download (bool, optional)** – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.\n",
        "transform (callable, optional) – A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop\n",
        "* **target_transform (callable, optional)** – A function/transform that takes in the target and transforms it."
      ],
      "metadata": {
        "id": "QKdOHeiRaYye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to\n",
        "    train=True, # do we want the training dataset?\n",
        "    download=True, # do we want to download?\n",
        "    transform=torchvision.transforms.ToTensor(), # how to transform the data\n",
        "    target_transform=None # how do we want to transform the labels/target\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        "    target_transform=None\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "RHElLWNrXinD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "l-ck3GS7c4T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the first training data- this will output the data as tensors (C x H x W) NOTE: grey scale images only have 1 color channel\n",
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "id": "lXZdKF5HdkNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "id": "Q4qCvtYvdzXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = train_data.class_to_idx\n",
        "class_to_idx"
      ],
      "metadata": {
        "id": "fJd2WzMseefo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets"
      ],
      "metadata": {
        "id": "WSzPIjWdekC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shape of our image\n",
        "print(f\"Image Shape: {image.shape} --> [color_channels, height, width], Image Label: {class_names[label]}\")"
      ],
      "metadata": {
        "id": "H6fZ_fyoeq2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing our data"
      ],
      "metadata": {
        "id": "uQfbCtS7lfZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "print(f\"Image Shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\") # had to remove a dimension so it would plot\n",
        "plt.title(class_names[label])\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(image.squeeze())\n",
        "# image"
      ],
      "metadata": {
        "id": "PwOKYwB6e0hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "row, cols = 4, 4\n",
        "for i in range(1, row * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    img, label = train_data[random_idx]\n",
        "    fig.add_subplot(row, cols, i)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    plt.title(class_names[label])\n",
        "    plt.axis(False)"
      ],
      "metadata": {
        "id": "hDfKBAFImDpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Input/Output shapes of Data"
      ],
      "metadata": {
        "id": "10izM2atmHmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Image Shape: {image.shape}\")\n",
        "print(f\"Image Label: {class_names[label]}\")"
      ],
      "metadata": {
        "id": "K2L4jBuJni32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing data"
      ],
      "metadata": {
        "id": "7u2cNaImnOVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "print(f\"Image Shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze(), cmap=\"plasma\") # had to remove a dimension so it would plot b/c shape issue (1, 28, 28) and output data is not correlating with image size it is looking for, in this case it expects color channels to be last the squeze gets rid of the 1 in [1, 28, 28]\n",
        "plt.title(class_names[label])\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "YSrg8z6Zmh2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import colormaps\n",
        "list(colormaps)"
      ],
      "metadata": {
        "id": "aCRF6ZHJnmub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "row, cols = 4, 4\n",
        "for i in range(1, row * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    img, label = train_data[random_idx]\n",
        "    fig.add_subplot(row, cols, i)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    plt.title(class_names[label])\n",
        "    plt.axis(False);"
      ],
      "metadata": {
        "id": "T36RTbmzpAph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can these items of clothing (images) could be modelled with linear lines only? Or is it the case we will have to introduce some non-linearity? Just a thought."
      ],
      "metadata": {
        "id": "cVMahsnuzXRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data"
      ],
      "metadata": {
        "id": "5qq_Hwpg0fTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare DataLoader\n",
        "\n",
        "Right now, our data is in the form of PyTorch Datasets.\n",
        "\n",
        "DataLoader turns our dataset into Python iterable.\n",
        "\n",
        "More specifically, we want to turn our data into batches (or mini-batches)\n",
        "\n",
        "Q) Why do we do this?\n",
        "\n",
        "A) The data takes up memory, and we have 60,000 training mages and 10,000 testing images. To alleviate this memeory load, we break the data up into batches. More Specifically:\n",
        "\n",
        "1. It is more computationally efficient, as in, your computing hardware may not be able to look at (store in memory) 60000 images at once. Thus we brak these images up into batches of 32 (batch_size=32). This is a very common batch size.\n",
        "2. It gives our neural network more chances to update it's gradients per epoch. See video by Andrew ng: https://www.youtube.com/watch?v=4qJaSmvhxi8 for more info about this.\n",
        "3. One parameter in the DataLoader is `shuffle`. We want to be able to shuffle the data incase there is some pre-determined order to our data and this helps randomize the images the training loop sees without that order grafted onto our model, thus producing a poor model. We don't want our model to 'memorize' the data.\n"
      ],
      "metadata": {
        "id": "pGzSw4vx0HOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batchify our dataset\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 32\n",
        "# Turn our datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False) # we don't shuffle the test dataset\n",
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "vJtjRBTGyCeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check out what we've created\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
      ],
      "metadata": {
        "id": "8Eti1m2c5lEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check out what is inside the training dataloader"
      ],
      "metadata": {
        "id": "3GNHzGkK7RkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ],
      "metadata": {
        "id": "gYfpMFEp77B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note above, the color channels are first"
      ],
      "metadata": {
        "id": "xxrfWLru80Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
        "image, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
        "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)\n",
        "print(f\"Image Shape: {image.shape}\")\n",
        "print(f\"Label: {label}, label_size: {label.shape}\")"
      ],
      "metadata": {
        "id": "sVHjYEYn7hom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 0: Build a baseline model\n",
        "\n",
        "When starting to build a series of machine learning modelling experiments, it's best practice to start with a *baseline model*\n",
        "\n",
        "A baseline model in a model you will try to improve upon with subsequent models/expt's\n",
        "\n",
        "AKA: start simply and add/ experiment with complexity when necessary 🧪"
      ],
      "metadata": {
        "id": "PTbioSbr_o4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flattened layer\n",
        "flatten_model = torch.nn.Flatten()\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "x.shape\n",
        "# Flatten the sample\n",
        "output = flatten_model(x)\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [color_chanells, height*width]\")"
      ],
      "metadata": {
        "id": "gloOV99O6ZcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see the batch size and the product of 78x78"
      ],
      "metadata": {
        "id": "WC7JUsO1EHU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class FashionMNISTModelV0(nn.Module):  # Inherit from nn.Module\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(  # Correct the attribute name\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer_stack(x)  # Use the correct attribute name\n"
      ],
      "metadata": {
        "id": "Y22h9kmFCvT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0 = FashionMNISTModelV0(\n",
        "    input_shape=784,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)\n",
        ").to(\"cpu\")  # Move model to CPU\n",
        "print(model_0)"
      ],
      "metadata": {
        "id": "1j1MvAkMD2Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x = torch.rand([1, 1, 28, 28])\n",
        "model_0(dummy_x)"
      ],
      "metadata": {
        "id": "Vb_hkNP0HGEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "betRYbtjKehC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup loss, optimizer and evaluation metrics\n",
        "\n",
        "* Loss function- since we're working with multi-class data, our loss function will be `nn.CrossEntropyLoss()`\n",
        "* Optimizer - our optimizer `torch.optim.SGD()`\n",
        "* Evaluation Metric- since this is a classification problem, we'll use Accuracy\n"
      ],
      "metadata": {
        "id": "eaZd8bqvU16r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper function for accuracy from learn PyTorch.repo\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download....\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "id": "oJGAtRJ6LOop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn"
      ],
      "metadata": {
        "id": "y3IH1I7aW57i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_fn(torch.tensor([[0.2, 0.5, 0.3]]), torch.tensor([2]))"
      ],
      "metadata": {
        "id": "oyljoL1yXJO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss and optimizer functions\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "i8A9yWJ8XVJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a function to time our experiments\n",
        "\n",
        "We need to be cognizant of the fact that Machine/Deep Learning is very experimental. These experiments can be very costly with respect to the resources that they require in terms of memory and GPU usage. When scaled up to very large jobs, you might find that the added complexity also comes at the cost of <u>*time*</u>.\n",
        "\n",
        "Thus two main things we'll keep track  (we'll find there is a trade-off between these):\n",
        "1. Model's performance (loss and accuracy values, etc.)\n",
        "2. How fast model runs.\n",
        "\n",
        "We are already tracking our model wrt lossfunction and accuracy, let's explore the time dimension below. Since we'll be using `timeit`, here's where to find the documentation: https://docs.python.org/3/library/timeit.html\n",
        "\n",
        "The default timer, which is always `time.perf_counter()`, returns float seconds. An alternative, `time.perf_counter_ns`, returns integer nanoseconds.\n",
        "\n",
        "```python\n",
        "class timeit.Timer(stmt='pass', setup='pass', timer=<timer function>, globals=None)\n",
        "```"
      ],
      "metadata": {
        "id": "NHeDfxwg026L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "\n",
        "    '''\n",
        "    prints difference between start and end time\n",
        "    '''\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time"
      ],
      "metadata": {
        "id": "SyZifpseX1Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = timer()\n",
        "end_time = timer()\n",
        "print_train_time(start=start_time, end=end_time, device=None)\n",
        "print_train_time(start=start_time, end=end_time, device=\"cpu\")"
      ],
      "metadata": {
        "id": "aFYX6Z9i4tmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a training loop and training a model on batches of the data\n",
        "remember: the optimizer will update a model's parameters once per batch rather than one per epoch....\n",
        "\n",
        "key steps:\n",
        "1. Loop through the epochs\n",
        "2. Loop through training batches, perform training steps, calculate the loss *per batch*\n",
        "3. Loop through testing batches, perform testing steps, calculate the loss *per batch*\n",
        "4. print out what's happening\n",
        "5. time it all\n",
        "\n",
        "### NOTE Below we are iterating and keeping count of the accumulated `train_loss` below. Here are some specific details about the use of the `enumerate()` function and how it is being used:\n",
        "\n",
        "1. `train_dataloader`: This is an iterable object, such as a PyTorch `DataLoader`, which provides batches of data (`X`) and corresponding labels (`y`) for training our machine learning model.\n",
        "\n",
        "2. `enumerate(train_dataloader)`: The `enumerate` function iterates over `train_dataloader` and, in addition to yielding each batch of data `(X, y)`, it also provides an index (`batch`) for the current iteration. The `batch` variable represents the batch number, starting from 0 by default.\n",
        "\n",
        "### Purpose of enumerate in this loop:\n",
        "1. Tracking batch indices: The `batch` variable allows you to keep track of which batch is being processed. This can be useful for:\n",
        "\n",
        "* Logging or debugging (e.g., printing the batch number during training).\n",
        "* Performing specific actions at certain batch intervals (e.g., saving a model every 100 batches).\n",
        "* Analyzing batch-specific metrics.\n",
        "2. Improved readability: By using `enumerate`, you don't have to manually maintain a counter variable and increment it in each iteration. It keeps the code concise and clean.\n",
        "\n",
        "**Here’s how it might be used in practice in the generic sense:**\n",
        "```\n",
        "for batch, (X, y) in enumerate(train_dataloader):\n",
        "    print(f\"Processing batch {batch}\")\n",
        "    # Perform training step\n",
        "    output = model(X)\n",
        "    loss = loss_function(output, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "```\n",
        "Here:\n",
        "* `batch` keeps track of the current batch number.\n",
        "* `(X, y)` contains the features (independent vars) and labels (target) for that batch.\n",
        "#### Using `enumerate` is a common practice in Python loops whenever you need both the index and the elements of an iterable."
      ],
      "metadata": {
        "id": "QVTEGJUq5dVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tqdm for progress bar- .auto recognizes programming environment\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set seed and start timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "#Set the number of epochs (keep small for faster training time)\n",
        "epochs = 3\n",
        "\n",
        "# Create training and test loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    # Training\n",
        "    train_loss = 0\n",
        "    # Add loop through training batches\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "        model_0.train()\n",
        "        # Forward pass\n",
        "        y_pred = model_0(X)\n",
        "\n",
        "        # Calculate loss (per batch)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        # accumulate the training loss per batch\n",
        "        train_loss += loss\n",
        "        # Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "        # Loss backward\n",
        "        loss.backward()\n",
        "        # Optimizer step\n",
        "        optimizer.step() # updating our models parameters per batch\n",
        "\n",
        "        # Print out how many samples have been seen\n",
        "        if batch % 400 == 0:\n",
        "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "    # Come back to the epoch loop and divide total train loss by length of train dataloader\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    # Testing\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model_0.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X_test, y_test in test_dataloader:\n",
        "            # Forward pass\n",
        "            test_pred = model_0(X_test)\n",
        "\n",
        "            # calculate the loss (accumulated)\n",
        "            test_loss += loss_fn(test_pred, y_test)\n",
        "\n",
        "            # Calculate the accuracy (accumulated)\n",
        "            test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1)) # getting the logit value with the highest idx and that is the pred label\n",
        "\n",
        "        # Scale loss and acc\n",
        "        test_loss /= len(test_dataloader)\n",
        "        #Calculate the test accuracy\n",
        "        test_acc /= len(test_dataloader)\n",
        "\n",
        "    # print out what's happening\n",
        "    print(f\"\\nTrain loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}\")\n",
        "\n",
        "    # Calculate training time\n",
        "    train_time_end_on_cpu = timer()\n",
        "    total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, end=train_time_end_on_cpu, device=str(next(model_0.parameters()).device))\n"
      ],
      "metadata": {
        "id": "oV9dsuxs47zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model_0 and make predictions: This is us functionalizing this step for use on any model\n",
        "\n",
        "Also note that the argmax is finding the index of the highest logit value. The raw outputs of our model are logits and if we ant to convert them into labels we could use the softmax function but here we use the argmax."
      ],
      "metadata": {
        "id": "BTTZ4tDlLj9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, accuracy_fn):\n",
        "    '''\n",
        "    Returns a dictionary containing the results of model predicting on data_loader\n",
        "    '''\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in tqdm(data_loader):\n",
        "            # Make predictions\n",
        "            y_pred = model(X) # note, don't have to specify model, see above\n",
        "\n",
        "            # Accumulate the loss and acc values per batch\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "        # Scale loss and acc to find the average loss/acc per batch\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n",
        "\n",
        "# Calculate model 0 ewsuts on test dataset\n",
        "model_0_results = eval_model(model=model_0,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn)\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "YjhQG-ftHB-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up device agnostic code to run on gpu"
      ],
      "metadata": {
        "id": "O-oi9OXHPkcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "Wu9TWAbiOvQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "OZHoKP0XQsel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our model did ok using no nonliearity, however now we will employ some non-liear functions\n",
        "\n",
        "In past notebooks we've learned about the power of non-liearity in evaluating data. Let's put that to the test."
      ],
      "metadata": {
        "id": "r8oK8J2YTivu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class FashionMNISTModelV1(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        \"\"\"\n",
        "        Initializes the FashionMNISTModelV1 model.\n",
        "\n",
        "        Args:\n",
        "            input_shape (int): The number of input features (e.g., 28*28 for flattened images).\n",
        "            hidden_units (int): The number of hidden units in the first linear layer.\n",
        "            output_shape (int): The number of output features (e.g., 10 for FashionMNIST classes).\n",
        "        \"\"\"\n",
        "        super(FashionMNISTModelV1, self).__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(),  # Flatten the inputs into a single vector\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the model.\n",
        "        \"\"\"\n",
        "        return self.layer_stack(x)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w6SZkzf3Qvpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "input_shape = 28 * 28  # For flattened 28x28 images\n",
        "hidden_units = 10  # Number of hidden units\n",
        "output_shape = 10  # Number of classes in FashionMNIST (e.g., 10 classes)\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model\n",
        "torch.manual_seed(42)\n",
        "model_1 = FashionMNISTModelV1(input_shape=input_shape, hidden_units=hidden_units, output_shape=output_shape).to(device)\n",
        "\n",
        "# Verify model device\n",
        "print(next(model_1.parameters()).device)\n",
        "\n"
      ],
      "metadata": {
        "id": "jsvdDGL1Z74W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss and optimizer functions\n",
        "loss_fn = nn.CrossEntropyLoss() # measures how far from test values our model is\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1) # tries to update our models parameters to improve performance/ reduce loss"
      ],
      "metadata": {
        "id": "iyOtX1fjXwYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funtionalize training/evaluation loop\n",
        "\n",
        "Let's create a function for:\n",
        "* training loop - `train_step()`\n",
        "* testing loop - `test_step()`"
      ],
      "metadata": {
        "id": "-vRvEtN9gteo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device):\n",
        "\n",
        "    \"\"\"\n",
        "    Performs a training step where the model learns on data_loader.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to train.\n",
        "        data_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
        "        loss_fn (torch.nn.Module): Loss function to optimize.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer for model parameters.\n",
        "        accuracy_fn (callable): Function to calculate accuracy.\n",
        "        device (torch.device): Device to run training on (e.g., 'cuda' or 'cpu').\n",
        "    \"\"\"\n",
        "    # Put the model into training mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize tracking metrics\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    # Loop through the training batches\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "\n",
        "        # Move data to target device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Forward pass - outputs raw logits from the model\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # Calculate the loss per batch\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()  # Add scalar value to train_loss\n",
        "\n",
        "        # Calculate the accuracy per batch\n",
        "        train_acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))  # Converts logits to labels\n",
        "\n",
        "        # Zero gradients for the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimizer step - update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average loss and accuracy across all batches\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "6FCHP4_Igkgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device):\n",
        "    \"\"\"\n",
        "    Performs a testing loop on the given model over the data_loader.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to test.\n",
        "        data_loader (torch.utils.data.DataLoader): DataLoader for test data.\n",
        "        loss_fn (torch.nn.Module): Loss function to calculate the loss.\n",
        "        accuracy_fn (function): Function to calculate accuracy.\n",
        "        device (torch.device): Device to run the testing on.\n",
        "    \"\"\"\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval()\n",
        "\n",
        "    # Turn on inference mode context manager\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send the data to target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass (raw logits)\n",
        "            test_pred = model(X)\n",
        "\n",
        "            # Calculate the loss (accumulated)\n",
        "            test_loss += float(loss_fn(test_pred, y))\n",
        "\n",
        "            # Calculate the accuracy (accumulated)\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "        # Adjust metrics\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "\n",
        "        # Print results (adjust based on accuracy_fn behavior)\n",
        "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "apQjXC5xqiqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's put our functions to use"
      ],
      "metadata": {
        "id": "U6EeP5L8xcag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper function for accuracy from learn PyTorch.repo\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download....\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "id": "V89KC-oy0aw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn"
      ],
      "metadata": {
        "id": "X0f7ZZgK0dW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_start_time_on_gpu = timer()\n",
        "\n",
        "# Set epochs\n",
        "epochs = 3\n",
        "\n",
        "# Create optimimization loop using train_step() and test_step()\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    train_step(model=model_1,\n",
        "               data_loader=train_dataloader,\n",
        "               loss_fn=loss_fn,\n",
        "               optimizer=optimizer,\n",
        "               accuracy_fn=accuracy_fn,\n",
        "               device=device)\n",
        "    test_step(model=model_1,\n",
        "              data_loader=test_dataloader,\n",
        "              loss_fn=loss_fn,\n",
        "              accuracy_fn=accuracy_fn,\n",
        "              device=device)\n",
        "\n",
        "train_end_time_on_gpu = timer()\n",
        "total_train_time_model_1 = print_train_time(start=train_start_time_on_gpu,      end=train_end_time_on_gpu, device=device)"
      ],
      "metadata": {
        "id": "Bmy45eCywivy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "bva9aS7vzoct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_time_model_0, total_train_time_model_1"
      ],
      "metadata": {
        "id": "yJBYQdJ56nm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### It's interesting to see that the model ran on the GPU took about the same time to run as the one on the cpu! This is likely down to the fact that this model isn't that large, and our code for setting up the layers in the CNN is also not that complex.\n",
        "\n",
        "> **Note**: Sometimes, depending on your data/hardware you might find that your model trains faster on a CPU than a GPU\n",
        "\n",
        "> Why is this?\n",
        "> 1. It could be that the overheadfor copying data/model to and from the GPU outweighs the compute benefits offered by the GPU. So there is some extra time involved in copying the data to the GPU.\n",
        "> 2. The hardware you're using has a better CPU in terms of its capability than the GPU.\n",
        "\n",
        "See this article about gpu's:\n",
        "https://horace.io/brrr_intro.html"
      ],
      "metadata": {
        "id": "ijMHz2aYO3YN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now to evaluate our model we need to remember to put the results on the gpu!"
      ],
      "metadata": {
        "id": "PslYwQybTnev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, accuracy_fn, device=device):\n",
        "    '''\n",
        "    Returns a dictionary containing the results of model predicting on data_loader\n",
        "    '''\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in tqdm(data_loader):\n",
        "            # Make our data device agnostic\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            # Make predictions\n",
        "            y_pred = model(X) # note, don't have to specify model, see above\n",
        "\n",
        "            # Accumulate the loss and acc values per batch\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "        # Scale loss and acc to find the average loss/acc per batch\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n"
      ],
      "metadata": {
        "id": "7Jw4XOfATlIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_1 results dictionary\n",
        "model_1_results = eval_model(model=model_1,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn,\n",
        "                             device=device)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "Zf6ld4AgO13k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "compared with"
      ],
      "metadata": {
        "id": "YzhXgPNwYoXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_results"
      ],
      "metadata": {
        "id": "cH6RnIqvYLHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: Building Convolutional Neural Netowrk (CNN)\n",
        "\n",
        "CNN's arre known as ConvNET's\n",
        "\n",
        "CNN's are known for their capabilities to find patterns in visual data\n",
        "\n",
        "#### Below is a table outlining critical pieces of a CNN"
      ],
      "metadata": {
        "id": "CTlyzNRvZASR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Hyperparameter/Layer Type**         | **What does it do?**                                                                 | **Typical Values**                                                                                      |\n",
        "|----------------------------------------|-------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|\n",
        "| **Input image(s)**                     | Target images you'd like to discover patterns in                                     | Whatever you can take a photo (or video) of                                                            |\n",
        "| **Input layer**                        | Takes in target images and preprocesses them for further layers                     | `input_shape = [batch_size, image_height, image_width, color_channels]` (channels last) or              |\n",
        "|                                        |                                                                                     | `input_shape = [batch_size, color_channels, image_height, image_width]` (channels first)               |\n",
        "| **Convolution layer**                  | Extracts/learns the most important features from target images                      | Multiple, can create with `torch.nn.ConvXd()` (X can be multiple values)                               |\n",
        "| **Hidden activation/non-linear activation** | Adds non-linearity to learned features (non-straight lines)                         | Usually ReLU (`torch.nn.ReLU()`), though can be many more                                              |\n",
        "| **Pooling layer**                      | Reduces the dimensionality of learned image features                                | Max (`torch.nn.MaxPool2d()`) or Average (`torch.nn.AvgPool2d()`)                                       |\n",
        "| **Output layer/linear layer**          | Takes learned features and outputs them in shape of target labels                   | `torch.nn.Linear(out_features=[number_of_classes])` (e.g., 3 for pizza, steak, or sushi)               |\n",
        "| **Output activation**                  | Converts output logits to prediction probabilities                                  | `torch.sigmoid()` (binary classification) or `torch.softmax()` (multi-class classification)            |\n"
      ],
      "metadata": {
        "id": "OWRL_NVoadxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find out what's happening inside a CNN, see this website:\n",
        "\n",
        "https://poloclub.github.io/cnn-explainer/"
      ],
      "metadata": {
        "id": "cMCneqygijHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CNN\n",
        "class FashionMNISTModelV2(nn.Module):\n",
        "    '''\n",
        "    Model architecture that replicates that replicates the TinyVGG\n",
        "    model from the CNN Explainer website.\n",
        "    '''\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1), # values we can set ourselves- hyperparameters\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2)\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "            out_channels=hidden_units,\n",
        "            kernel_size=3,\n",
        "            padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "            out_channels=hidden_units,\n",
        "            kernel_size=3,\n",
        "            padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=hidden_units*0,\n",
        "            out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block_1(x)\n",
        "        print(x.shape)\n",
        "        x = self.conv_block_2(x)\n",
        "        print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WKKwG535Yr_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_2 = FashionMNISTModelV2(input_shape=1, # since we're working with greyscale images the coloe channel is 1\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names)).to(device)"
      ],
      "metadata": {
        "id": "oeotTbRbedDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stepping through `nn.Conv2d()`\n",
        "```\n",
        "classtorch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)```\n",
        "\n",
        "docs:\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n"
      ],
      "metadata": {
        "id": "VvmW1-swHqWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.state_dict()"
      ],
      "metadata": {
        "id": "CthfgPVqKTdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a batch of images\n",
        "images = torch.randn(size=(32, 3, 64, 64))\n",
        "test_image = images[0]\n",
        "\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "print(f\"Single image shape: {test_image.shape}\")\n",
        "print(f\"Test image:\\n{test_image}\")"
      ],
      "metadata": {
        "id": "bDLEGTIXHI8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a single conv2d layer\n",
        "conv_layer = nn.Conv2d(in_channels=3,\n",
        "                       out_channels=10,\n",
        "                       kernel_size=3, # kernel also known as a filter (3x3)\n",
        "                       stride=1,\n",
        "                       padding=0)\n",
        "\n",
        "# pass the data through the convolutional layer\n",
        "conv_output = conv_layer(test_image.unsqueeze(0))\n",
        "conv_output"
      ],
      "metadata": {
        "id": "iXWH0cNkKIzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pq2tzO9TRrNu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}