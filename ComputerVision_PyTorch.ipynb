{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOzdCf7bYIt5Ojl6tC24y0y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/PyTorch/blob/main/ComputerVision_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Vision- Using PYTorch\n",
        "\n",
        "**Basis**\n",
        "\n",
        "pixels are read as RGB colors and turned into --> numbers (tensors) or `numerical encoding` --> model (algorithm) --> output probability that the image is X ot Y or Z\n",
        "\n",
        "**Details**\n",
        " Tensors contain the following information:\n",
        " 1. Width of image\n",
        " 2. Height of image\n",
        " 3. Color channels == 3 (RGB)\n",
        " depending on what algorithm you're working with data as tensors whose ID is as follows:\n",
        "\n",
        " [batch_size, height, width, color_channels] OR [batch_size, color_channels, height, width]\n",
        "\n",
        " These will be mainly CNN models\n",
        "\n",
        " We will be working with `torch.nn.Conv2d`\n",
        "\n",
        " ## Computer version libraries in PyTorch\n",
        "\n",
        "* `torchvision`- base domain library for PyTorch computer vision-\n",
        "  https://pytorch.org/vision/stable/index.html\n",
        "* `torchvision.datassets`get datasets and loading functions here:\n",
        "  https://pytorch.org/vision/stable/datasets.html#built-in-datasets\n",
        "* `torchvision.models` get pre-trained computer vision models i.e. have pretrained weights, etc. that you can leverage for your own problems.\n",
        "* `torchvision.transforms`- functions for manipulating your vision data (images) to be suitable for use with an ML model.\n",
        "* `torch.utils.Dataset`- Base dataset class for PyTorch.\n",
        "* `torch.utils.data.DataLoader` - Creates a Python iterable over a dataset\n",
        "\n",
        "Torchvision supports common computer vision transformations in the torchvision.transforms and torchvision.transforms.v2 modules. Transforms can be used to transform or augment data for training or inference of different tasks (image classification, detection, segmentation, video classification).\n",
        "\n",
        "* PIL is the Python Imaging Library by Fredrik Lundh and contributors.\n",
        "\n",
        "### torchvision.datasets\n",
        "\n",
        "All datasets are subclasses of torch.utils.data.Dataset i.e, they have __getitem__ and __len__ methods implemented. Hence, they can all be passed to a torch.utils.data.DataLoader which can load multiple samples parallelly using torch.multiprocessing workers. For example:\n",
        "```\n",
        "imagenet_data = torchvision.datasets.ImageNet('path/to/imagenet_root/')\n",
        "data_loader = torch.utils.data.DataLoader(imagenet_data,\n",
        "                                          batch_size=4,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=args.nThreads)\n",
        "```"
      ],
      "metadata": {
        "id": "nPbopb9gp_qY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peQatlzGl9MS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting a dataset\n",
        "\n",
        "we will be using `fashion.mnist` datset- greyscale images of clothing\n",
        "basic dataset for implementation here\n",
        "\n",
        "Be aware that IMAGENET  is the gold standard for computer vision evaluations"
      ],
      "metadata": {
        "id": "vvnEhTotYExd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torchvision.datasets.FashionMNIST(root: str, train: bool = True, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) â†’ None[source]`\n",
        "\n",
        "### Fashion-MNIST Dataset.\n",
        "\n",
        "Parameters:\n",
        "* **root (string)** â€“ Root directory of dataset where FashionMNIST/processed/training.pt and FashionMNIST/processed/test.pt exist.\n",
        "* **train (bool, optional)** â€“ If True, creates dataset from training.pt, otherwise from test.pt.\n",
        "* **download (bool, optional)** â€“ If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.\n",
        "transform (callable, optional) â€“ A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop\n",
        "* **target_transform (callable, optional)** â€“ A function/transform that takes in the target and transforms it."
      ],
      "metadata": {
        "id": "QKdOHeiRaYye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to\n",
        "    train=True, # do we want the training dataset?\n",
        "    download=True, # do we want to download?\n",
        "    transform=torchvision.transforms.ToTensor(), # how to transform the data\n",
        "    target_transform=None # how do we want to transform the labels/target\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        "    target_transform=None\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "RHElLWNrXinD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "l-ck3GS7c4T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the first training data- this will output the data as tensors (C x H x W) NOTE: grey scale images only have 1 color channel\n",
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "id": "lXZdKF5HdkNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "id": "Q4qCvtYvdzXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = train_data.class_to_idx\n",
        "class_to_idx"
      ],
      "metadata": {
        "id": "fJd2WzMseefo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets"
      ],
      "metadata": {
        "id": "WSzPIjWdekC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shape of our image\n",
        "print(f\"Image Shape: {image.shape} --> [color_channels, height, width], Image Label: {class_names[label]}\")"
      ],
      "metadata": {
        "id": "H6fZ_fyoeq2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing our data"
      ],
      "metadata": {
        "id": "uQfbCtS7lfZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "print(f\"Image Shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\") # had to remove a dimension so it would plot\n",
        "plt.title(class_names[label])\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(image.squeeze())\n",
        "# image"
      ],
      "metadata": {
        "id": "PwOKYwB6e0hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "row, cols = 4, 4\n",
        "for i in range(1, row * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    img, label = train_data[random_idx]\n",
        "    fig.add_subplot(row, cols, i)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    plt.title(class_names[label])\n",
        "    plt.axis(False)"
      ],
      "metadata": {
        "id": "hDfKBAFImDpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Input/Output shapes of Data"
      ],
      "metadata": {
        "id": "10izM2atmHmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Image Shape: {image.shape}\")\n",
        "print(f\"Image Label: {class_names[label]}\")"
      ],
      "metadata": {
        "id": "K2L4jBuJni32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing data"
      ],
      "metadata": {
        "id": "7u2cNaImnOVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "print(f\"Image Shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze(), cmap=\"plasma\") # had to remove a dimension so it would plot b/c shape issue (1, 28, 28) and output data is not correlating with image size it is looking for, in this case it expects color channels to be last the squeze gets rid of the 1 in [1, 28, 28]\n",
        "plt.title(class_names[label])\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "YSrg8z6Zmh2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import colormaps\n",
        "list(colormaps)"
      ],
      "metadata": {
        "id": "aCRF6ZHJnmub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "row, cols = 4, 4\n",
        "for i in range(1, row * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    img, label = train_data[random_idx]\n",
        "    fig.add_subplot(row, cols, i)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    plt.title(class_names[label])\n",
        "    plt.axis(False);"
      ],
      "metadata": {
        "id": "T36RTbmzpAph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can these items of clothing (images) could be modelled with linear lines only? Or is it the case we will have to introduce some non-linearity? Just a thought."
      ],
      "metadata": {
        "id": "cVMahsnuzXRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data"
      ],
      "metadata": {
        "id": "5qq_Hwpg0fTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare DataLoader\n",
        "\n",
        "Right now, our data is in the form of PyTorch Datasets.\n",
        "\n",
        "DataLoader turns our dataset into Python iterable.\n",
        "\n",
        "More specifically, we want to turn our data into batches (or mini-batches)\n",
        "\n",
        "Q) Why do we do this?\n",
        "A) The data takes up memory, and we have 60,000 training mages and 10,000 testing images. To alleviate this memeory load, we break the data up into batches. More Specifically:\n",
        "\n",
        "1. It is more computationally efficient, as in, your computing hardware may not be able to look at (store in memory) 60000 images at once. Thus we brak these images up into batches of 32 (batch_size=32). This is a very common batch size.\n",
        "2. It gives our neural network more chances to update it's gradients per epoch. See video by Andrew ng: https://www.youtube.com/watch?v=4qJaSmvhxi8 for more info about this.\n",
        "3. One parameter in the DataLoader is `shuffle`. We want to be able to shuffle the data incase there is some pre-determined order to our data and this helps randomize the images the training loop sees without that order grafted onto our model, thus producing a poor model. We don't want our model to 'memorize' the data.\n"
      ],
      "metadata": {
        "id": "pGzSw4vx0HOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batchify our dataset\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 32\n",
        "# Turn our datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False) # we don't shuffle the test dataset\n",
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "vJtjRBTGyCeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check out what we've created\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
      ],
      "metadata": {
        "id": "8Eti1m2c5lEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check out what is inside the training dataloader"
      ],
      "metadata": {
        "id": "3GNHzGkK7RkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ],
      "metadata": {
        "id": "gYfpMFEp77B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note above, the color channels are first"
      ],
      "metadata": {
        "id": "xxrfWLru80Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
        "image, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
        "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)\n",
        "print(f\"Image Shape: {image.shape}\")\n",
        "print(f\"Label: {label}, label_size: {label.shape}\")"
      ],
      "metadata": {
        "id": "sVHjYEYn7hom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 0: Build a baseline model\n",
        "\n",
        "When starting to build a series of machine learning modelling experiments, it's best practice to start with a *baseline model*\n",
        "\n",
        "A baseline model in a model you will try to improve upon with subsequent models/expt's\n",
        "\n",
        "AKA: start simply and add/ experiment with complexity when necessary ðŸ§ª"
      ],
      "metadata": {
        "id": "PTbioSbr_o4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flattened layer\n",
        "flatten_model = torch.nn.Flatten()\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "x.shape\n",
        "# Flatten the sample\n",
        "output = flatten_model(x)\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [color_chanells, height*width]\")"
      ],
      "metadata": {
        "id": "gloOV99O6ZcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see the batch size and the product of 78x78"
      ],
      "metadata": {
        "id": "WC7JUsO1EHU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class FashionMNISTModelV0(nn.Module):  # Inherit from nn.Module\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(  # Correct the attribute name\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer_stack(x)  # Use the correct attribute name\n"
      ],
      "metadata": {
        "id": "Y22h9kmFCvT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0 = FashionMNISTModelV0(\n",
        "    input_shape=784,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)\n",
        ").to(\"cpu\")  # Move model to CPU\n",
        "print(model_0)"
      ],
      "metadata": {
        "id": "1j1MvAkMD2Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x = torch.rand([1, 1, 28, 28])\n",
        "model_0(dummy_x)"
      ],
      "metadata": {
        "id": "Vb_hkNP0HGEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "betRYbtjKehC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup loss, optimizer and evaluation metrics\n",
        "\n",
        "* Loss function- since we're working with multi-class data, our loss function will be `nn.CrossEntropyLoss()`\n",
        "* Optimizer - our optimizer `torch.optim.SGD()`\n",
        "* Evaluation Metric- since this is a classification problem, we'll use Accuracy\n"
      ],
      "metadata": {
        "id": "eaZd8bqvU16r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper function for accuracy from learn PyTorch.repo\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download....\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "id": "oJGAtRJ6LOop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn"
      ],
      "metadata": {
        "id": "y3IH1I7aW57i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_fn(torch.tensor([[0.2, 0.5, 0.3]]), torch.tensor([2]))"
      ],
      "metadata": {
        "id": "oyljoL1yXJO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss and optimizer functions\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "i8A9yWJ8XVJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SyZifpseX1Xm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}