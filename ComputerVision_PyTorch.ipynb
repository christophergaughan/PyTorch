{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "mount_file_id": "1W8Zp4YeTlkN4ZdW_QWdvkimMZR-yjMQy",
      "authorship_tag": "ABX9TyOGSE3OaNSG2lVUS4Fy4XCy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/PyTorch/blob/main/ComputerVision_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Vision- Using PYTorch\n",
        "\n",
        "**Basis**\n",
        "\n",
        "pixels are read as RGB colors and turned into --> numbers (tensors) or `numerical encoding` --> model (algorithm) --> output probability that the image is X ot Y or Z\n",
        "\n",
        "**Details**\n",
        " Tensors contain the following information:\n",
        " 1. Width of image\n",
        " 2. Height of image\n",
        " 3. Color channels == 3 (RGB)\n",
        " depending on what algorithm you're working with data as tensors whose ID is as follows:\n",
        "\n",
        " [batch_size, height, width, color_channels] OR [batch_size, color_channels, height, width]\n",
        "\n",
        " These will be mainly CNN models\n",
        "\n",
        " We will be working with `torch.nn.Conv2d`\n",
        "\n",
        " ## Computer version libraries in PyTorch\n",
        "\n",
        "* `torchvision`- base domain library for PyTorch computer vision-\n",
        "  https://pytorch.org/vision/stable/index.html\n",
        "* `torchvision.datassets`get datasets and loading functions here:\n",
        "  https://pytorch.org/vision/stable/datasets.html#built-in-datasets\n",
        "* `torchvision.models` get pre-trained computer vision models i.e. have pretrained weights, etc. that you can leverage for your own problems.\n",
        "* `torchvision.transforms`- functions for manipulating your vision data (images) to be suitable for use with an ML model.\n",
        "* `torch.utils.Dataset`- Base dataset class for PyTorch.\n",
        "* `torch.utils.data.DataLoader` - Creates a Python iterable over a dataset\n",
        "\n",
        "Torchvision supports common computer vision transformations in the torchvision.transforms and torchvision.transforms.v2 modules. Transforms can be used to transform or augment data for training or inference of different tasks (image classification, detection, segmentation, video classification).\n",
        "\n",
        "* PIL is the Python Imaging Library by Fredrik Lundh and contributors.\n",
        "\n",
        "### torchvision.datasets\n",
        "\n",
        "All datasets are subclasses of torch.utils.data.Dataset i.e, they have __getitem__ and __len__ methods implemented. Hence, they can all be passed to a torch.utils.data.DataLoader which can load multiple samples parallelly using torch.multiprocessing workers. For example:\n",
        "```\n",
        "imagenet_data = torchvision.datasets.ImageNet('path/to/imagenet_root/')\n",
        "data_loader = torch.utils.data.DataLoader(imagenet_data,\n",
        "                                          batch_size=4,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=args.nThreads)\n",
        "```"
      ],
      "metadata": {
        "id": "nPbopb9gp_qY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peQatlzGl9MS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting a dataset\n",
        "\n",
        "we will be using `fashion.mnist` datset- greyscale images of clothing\n",
        "basic dataset for implementation here\n",
        "\n",
        "Be aware that IMAGENET  is the gold standard for computer vision evaluations"
      ],
      "metadata": {
        "id": "vvnEhTotYExd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torchvision.datasets.FashionMNIST(root: str, train: bool = True, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) → None[source]`\n",
        "\n",
        "### Fashion-MNIST Dataset.\n",
        "\n",
        "Parameters:\n",
        "* **root (string)** – Root directory of dataset where FashionMNIST/processed/training.pt and FashionMNIST/processed/test.pt exist.\n",
        "* **train (bool, optional)** – If True, creates dataset from training.pt, otherwise from test.pt.\n",
        "* **download (bool, optional)** – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.\n",
        "transform (callable, optional) – A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop\n",
        "* **target_transform (callable, optional)** – A function/transform that takes in the target and transforms it."
      ],
      "metadata": {
        "id": "QKdOHeiRaYye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to\n",
        "    train=True, # do we want the training dataset?\n",
        "    download=True, # do we want to download?\n",
        "    transform=torchvision.transforms.ToTensor(), # how to transform the data\n",
        "    target_transform=None # how do we want to transform the labels/target\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        "    target_transform=None\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "RHElLWNrXinD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "l-ck3GS7c4T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the first training data- this will output the data as tensors (C x H x W) NOTE: grey scale images only have 1 color channel\n",
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "id": "lXZdKF5HdkNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "id": "Q4qCvtYvdzXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = train_data.class_to_idx\n",
        "class_to_idx"
      ],
      "metadata": {
        "id": "fJd2WzMseefo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets"
      ],
      "metadata": {
        "id": "WSzPIjWdekC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shape of our image\n",
        "print(f\"Image Shape: {image.shape} --> [color_channels, height, width], Image Label: {class_names[label]}\")"
      ],
      "metadata": {
        "id": "H6fZ_fyoeq2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing our data"
      ],
      "metadata": {
        "id": "uQfbCtS7lfZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "print(f\"Image Shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\") # had to remove a dimension so it would plot\n",
        "plt.title(class_names[label])\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(image.squeeze())\n",
        "# image"
      ],
      "metadata": {
        "id": "PwOKYwB6e0hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "row, cols = 4, 4\n",
        "for i in range(1, row * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    img, label = train_data[random_idx]\n",
        "    fig.add_subplot(row, cols, i)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    plt.title(class_names[label])\n",
        "    plt.axis(False)"
      ],
      "metadata": {
        "id": "hDfKBAFImDpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Input/Output shapes of Data"
      ],
      "metadata": {
        "id": "10izM2atmHmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Image Shape: {image.shape}\")\n",
        "print(f\"Image Label: {class_names[label]}\")"
      ],
      "metadata": {
        "id": "K2L4jBuJni32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing data"
      ],
      "metadata": {
        "id": "7u2cNaImnOVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "print(f\"Image Shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze(), cmap=\"plasma\") # had to remove a dimension so it would plot b/c shape issue (1, 28, 28) and output data is not correlating with image size it is looking for, in this case it expects color channels to be last the squeze gets rid of the 1 in [1, 28, 28]\n",
        "plt.title(class_names[label])\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "YSrg8z6Zmh2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import colormaps\n",
        "list(colormaps)"
      ],
      "metadata": {
        "id": "aCRF6ZHJnmub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "row, cols = 4, 4\n",
        "for i in range(1, row * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    img, label = train_data[random_idx]\n",
        "    fig.add_subplot(row, cols, i)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    plt.title(class_names[label])\n",
        "    plt.axis(False);"
      ],
      "metadata": {
        "id": "T36RTbmzpAph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can these items of clothing (images) could be modelled with linear lines only? Or is it the case we will have to introduce some non-linearity? Just a thought."
      ],
      "metadata": {
        "id": "cVMahsnuzXRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data"
      ],
      "metadata": {
        "id": "5qq_Hwpg0fTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare DataLoader\n",
        "\n",
        "Right now, our data is in the form of PyTorch Datasets.\n",
        "\n",
        "DataLoader turns our dataset into Python iterable.\n",
        "\n",
        "More specifically, we want to turn our data into batches (or mini-batches)\n",
        "\n",
        "Q) Why do we do this?\n",
        "\n",
        "A) The data takes up memory, and we have 60,000 training mages and 10,000 testing images. To alleviate this memeory load, we break the data up into batches. More Specifically:\n",
        "\n",
        "1. It is more computationally efficient, as in, your computing hardware may not be able to look at (store in memory) 60000 images at once. Thus we brak these images up into batches of 32 (batch_size=32). This is a very common batch size.\n",
        "2. It gives our neural network more chances to update it's gradients per epoch. See video by Andrew ng: https://www.youtube.com/watch?v=4qJaSmvhxi8 for more info about this.\n",
        "3. One parameter in the DataLoader is `shuffle`. We want to be able to shuffle the data incase there is some pre-determined order to our data and this helps randomize the images the training loop sees without that order grafted onto our model, thus producing a poor model. We don't want our model to 'memorize' the data.\n"
      ],
      "metadata": {
        "id": "pGzSw4vx0HOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batchify our dataset\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 32\n",
        "# Turn our datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False) # we don't shuffle the test dataset\n",
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "vJtjRBTGyCeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check out what we've created\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
      ],
      "metadata": {
        "id": "8Eti1m2c5lEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check out what is inside the training dataloader"
      ],
      "metadata": {
        "id": "3GNHzGkK7RkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ],
      "metadata": {
        "id": "gYfpMFEp77B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note above, the color channels are first"
      ],
      "metadata": {
        "id": "xxrfWLru80Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
        "image, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
        "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)\n",
        "print(f\"Image Shape: {image.shape}\")\n",
        "print(f\"Label: {label}, label_size: {label.shape}\")"
      ],
      "metadata": {
        "id": "sVHjYEYn7hom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 0: Build a baseline model\n",
        "\n",
        "When starting to build a series of machine learning modelling experiments, it's best practice to start with a *baseline model*\n",
        "\n",
        "A baseline model in a model you will try to improve upon with subsequent models/expt's\n",
        "\n",
        "AKA: start simply and add/ experiment with complexity when necessary 🧪"
      ],
      "metadata": {
        "id": "PTbioSbr_o4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flattened layer\n",
        "flatten_model = torch.nn.Flatten()\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "x.shape\n",
        "# Flatten the sample\n",
        "output = flatten_model(x)\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [color_chanells, height*width]\")"
      ],
      "metadata": {
        "id": "gloOV99O6ZcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see the batch size and the product of 78x78"
      ],
      "metadata": {
        "id": "WC7JUsO1EHU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class FashionMNISTModelV0(nn.Module):  # Inherit from nn.Module\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(  # Correct the attribute name\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer_stack(x)  # Use the correct attribute name\n"
      ],
      "metadata": {
        "id": "Y22h9kmFCvT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0 = FashionMNISTModelV0(\n",
        "    input_shape=784,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)\n",
        ").to(\"cpu\")  # Move model to CPU\n",
        "print(model_0)"
      ],
      "metadata": {
        "id": "1j1MvAkMD2Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x = torch.rand([1, 1, 28, 28])\n",
        "model_0(dummy_x)"
      ],
      "metadata": {
        "id": "Vb_hkNP0HGEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "betRYbtjKehC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup loss, optimizer and evaluation metrics\n",
        "\n",
        "* Loss function- since we're working with multi-class data, our loss function will be `nn.CrossEntropyLoss()`\n",
        "* Optimizer - our optimizer `torch.optim.SGD()`\n",
        "* Evaluation Metric- since this is a classification problem, we'll use Accuracy\n"
      ],
      "metadata": {
        "id": "eaZd8bqvU16r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper function for accuracy from learn PyTorch.repo\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download....\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "id": "oJGAtRJ6LOop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn"
      ],
      "metadata": {
        "id": "y3IH1I7aW57i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_fn(torch.tensor([[0.2, 0.5, 0.3]]), torch.tensor([2]))"
      ],
      "metadata": {
        "id": "oyljoL1yXJO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss and optimizer functions\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "i8A9yWJ8XVJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a function to time our experiments\n",
        "\n",
        "We need to be cognizant of the fact that Machine/Deep Learning is very experimental. These experiments can be very costly with respect to the resources that they require in terms of memory and GPU usage. When scaled up to very large jobs, you might find that the added complexity also comes at the cost of <u>*time*</u>.\n",
        "\n",
        "Thus two main things we'll keep track  (we'll find there is a trade-off between these):\n",
        "1. Model's performance (loss and accuracy values, etc.)\n",
        "2. How fast model runs.\n",
        "\n",
        "We are already tracking our model wrt lossfunction and accuracy, let's explore the time dimension below. Since we'll be using `timeit`, here's where to find the documentation: https://docs.python.org/3/library/timeit.html\n",
        "\n",
        "The default timer, which is always `time.perf_counter()`, returns float seconds. An alternative, `time.perf_counter_ns`, returns integer nanoseconds.\n",
        "\n",
        "```python\n",
        "class timeit.Timer(stmt='pass', setup='pass', timer=<timer function>, globals=None)\n",
        "```"
      ],
      "metadata": {
        "id": "NHeDfxwg026L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "\n",
        "    '''\n",
        "    prints difference between start and end time\n",
        "    '''\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time"
      ],
      "metadata": {
        "id": "SyZifpseX1Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = timer()\n",
        "end_time = timer()\n",
        "print_train_time(start=start_time, end=end_time, device=None)\n",
        "print_train_time(start=start_time, end=end_time, device=\"cpu\")"
      ],
      "metadata": {
        "id": "aFYX6Z9i4tmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a training loop and training a model on batches of the data\n",
        "remember: the optimizer will update a model's parameters once per batch rather than one per epoch....\n",
        "\n",
        "key steps:\n",
        "1. Loop through the epochs\n",
        "2. Loop through training batches, perform training steps, calculate the loss *per batch*\n",
        "3. Loop through testing batches, perform testing steps, calculate the loss *per batch*\n",
        "4. print out what's happening\n",
        "5. time it all\n",
        "\n",
        "### NOTE Below we are iterating and keeping count of the accumulated `train_loss` below. Here are some specific details about the use of the `enumerate()` function and how it is being used:\n",
        "\n",
        "1. `train_dataloader`: This is an iterable object, such as a PyTorch `DataLoader`, which provides batches of data (`X`) and corresponding labels (`y`) for training our machine learning model.\n",
        "\n",
        "2. `enumerate(train_dataloader)`: The `enumerate` function iterates over `train_dataloader` and, in addition to yielding each batch of data `(X, y)`, it also provides an index (`batch`) for the current iteration. The `batch` variable represents the batch number, starting from 0 by default.\n",
        "\n",
        "### Purpose of enumerate in this loop:\n",
        "1. Tracking batch indices: The `batch` variable allows you to keep track of which batch is being processed. This can be useful for:\n",
        "\n",
        "* Logging or debugging (e.g., printing the batch number during training).\n",
        "* Performing specific actions at certain batch intervals (e.g., saving a model every 100 batches).\n",
        "* Analyzing batch-specific metrics.\n",
        "2. Improved readability: By using `enumerate`, you don't have to manually maintain a counter variable and increment it in each iteration. It keeps the code concise and clean.\n",
        "\n",
        "**Here’s how it might be used in practice in the generic sense:**\n",
        "```\n",
        "for batch, (X, y) in enumerate(train_dataloader):\n",
        "    print(f\"Processing batch {batch}\")\n",
        "    # Perform training step\n",
        "    output = model(X)\n",
        "    loss = loss_function(output, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "```\n",
        "Here:\n",
        "* `batch` keeps track of the current batch number.\n",
        "* `(X, y)` contains the features (independent vars) and labels (target) for that batch.\n",
        "#### Using `enumerate` is a common practice in Python loops whenever you need both the index and the elements of an iterable."
      ],
      "metadata": {
        "id": "QVTEGJUq5dVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set the seed and start the timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "# Set the number of epochs (we'll keep this small for faster training times)\n",
        "epochs = 3\n",
        "\n",
        "# Create training and testing loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    ### Training\n",
        "    train_loss = 0\n",
        "    # Add a loop to loop through training batches\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "        model_0.train()\n",
        "        # 1. Forward pass\n",
        "        y_pred = model_0(X)\n",
        "\n",
        "        # 2. Calculate loss (per batch)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss # accumulatively add up the loss per epoch\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print out how many samples have been seen\n",
        "        if batch % 400 == 0:\n",
        "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    ### Testing\n",
        "    # Setup variables for accumulatively adding up loss and accuracy\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model_0.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in test_dataloader:\n",
        "            # 1. Forward pass\n",
        "            test_pred = model_0(X)\n",
        "\n",
        "            # 2. Calculate loss (accumulatively)\n",
        "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
        "        # Divide total test loss by length of test dataloader (per batch)\n",
        "        test_loss /= len(test_dataloader)\n",
        "\n",
        "        # Divide total accuracy by length of test dataloader (per batch)\n",
        "        test_acc /= len(test_dataloader)\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
        "\n",
        "# Calculate training time\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, end=train_time_end_on_cpu,\n",
        "                                device=str(next(model_0.parameters()).device))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oV9dsuxs47zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model_0 and make predictions: This is us functionalizing this step for use on any model\n",
        "\n",
        "Also note that the argmax is finding the index of the highest logit value. The raw outputs of our model are logits and if we ant to convert them into labels we could use the softmax function but here we use the argmax."
      ],
      "metadata": {
        "id": "BTTZ4tDlLj9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, accuracy_fn):\n",
        "    '''\n",
        "    Returns a dictionary containing the results of model predicting on data_loader\n",
        "    '''\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in tqdm(data_loader):\n",
        "            # Make predictions\n",
        "            y_pred = model(X) # note, don't have to specify model, see above\n",
        "\n",
        "            # Accumulate the loss and acc values per batch\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "        # Scale loss and acc to find the average loss/acc per batch\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n",
        "\n",
        "# Calculate model 0 ewsuts on test dataset\n",
        "model_0_results = eval_model(model=model_0,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn)\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "YjhQG-ftHB-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up device agnostic code to run on gpu"
      ],
      "metadata": {
        "id": "O-oi9OXHPkcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "Wu9TWAbiOvQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "OZHoKP0XQsel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our model did ok using no nonliearity, however now we will employ some non-liear functions\n",
        "\n",
        "In past notebooks we've learned about the power of non-liearity in evaluating data. Let's put that to the test."
      ],
      "metadata": {
        "id": "r8oK8J2YTivu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class FashionMNISTModelV1(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        \"\"\"\n",
        "        Initializes the FashionMNISTModelV1 model.\n",
        "\n",
        "        Args:\n",
        "            input_shape (int): The number of input features (e.g., 28*28 for flattened images).\n",
        "            hidden_units (int): The number of hidden units in the first linear layer.\n",
        "            output_shape (int): The number of output features (e.g., 10 for FashionMNIST classes).\n",
        "        \"\"\"\n",
        "        super(FashionMNISTModelV1, self).__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(),  # Flatten the inputs into a single vector\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the model.\n",
        "        \"\"\"\n",
        "        return self.layer_stack(x)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w6SZkzf3Qvpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "input_shape = 28 * 28  # For flattened 28x28 images\n",
        "hidden_units = 10  # Number of hidden units\n",
        "output_shape = 10  # Number of classes in FashionMNIST (e.g., 10 classes)\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model\n",
        "torch.manual_seed(42)\n",
        "model_1 = FashionMNISTModelV1(input_shape=input_shape, hidden_units=hidden_units, output_shape=output_shape).to(device)\n",
        "\n",
        "# Verify model device\n",
        "print(next(model_1.parameters()).device)\n",
        "\n"
      ],
      "metadata": {
        "id": "jsvdDGL1Z74W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss and optimizer functions\n",
        "loss_fn = nn.CrossEntropyLoss() # measures how far from test values our model is\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1) # tries to update our models parameters to improve performance/ reduce loss"
      ],
      "metadata": {
        "id": "iyOtX1fjXwYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funtionalize training/evaluation loop\n",
        "\n",
        "Let's create a function for:\n",
        "* training loop - `train_step()`\n",
        "* testing loop - `test_step()`"
      ],
      "metadata": {
        "id": "-vRvEtN9gteo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device):\n",
        "\n",
        "    \"\"\"\n",
        "    Performs a training step where the model learns on data_loader.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to train.\n",
        "        data_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
        "        loss_fn (torch.nn.Module): Loss function to optimize.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer for model parameters.\n",
        "        accuracy_fn (callable): Function to calculate accuracy.\n",
        "        device (torch.device): Device to run training on (e.g., 'cuda' or 'cpu').\n",
        "    \"\"\"\n",
        "    # Put the model into training mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize tracking metrics\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    # Loop through the training batches\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "\n",
        "        # Move data to target device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Forward pass - outputs raw logits from the model\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # Calculate the loss per batch\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()  # Add scalar value to train_loss\n",
        "\n",
        "        # Calculate the accuracy per batch\n",
        "        train_acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))  # Converts logits to labels\n",
        "\n",
        "        # Zero gradients for the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimizer step - update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average loss and accuracy across all batches\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "6FCHP4_Igkgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device):\n",
        "    \"\"\"\n",
        "    Performs a testing loop on the given model over the data_loader.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to test.\n",
        "        data_loader (torch.utils.data.DataLoader): DataLoader for test data.\n",
        "        loss_fn (torch.nn.Module): Loss function to calculate the loss.\n",
        "        accuracy_fn (function): Function to calculate accuracy.\n",
        "        device (torch.device): Device to run the testing on.\n",
        "    \"\"\"\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval()\n",
        "\n",
        "    # Turn on inference mode context manager\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send the data to target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass (raw logits)\n",
        "            test_pred = model(X)\n",
        "\n",
        "            # Calculate the loss (accumulated)\n",
        "            test_loss += float(loss_fn(test_pred, y))\n",
        "\n",
        "            # Calculate the accuracy (accumulated)\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "        # Adjust metrics\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "\n",
        "        # Print results (adjust based on accuracy_fn behavior)\n",
        "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "apQjXC5xqiqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's put our functions to use"
      ],
      "metadata": {
        "id": "U6EeP5L8xcag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper function for accuracy from learn PyTorch.repo\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download....\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "id": "V89KC-oy0aw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn"
      ],
      "metadata": {
        "id": "X0f7ZZgK0dW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm.auto import tqdm\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "# Assume train_dataloader and test_dataloader are already defined\n",
        "\n",
        "# Example model\n",
        "model_1 = torch.nn.Sequential(\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(28 * 28, 128),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(128, 10)\n",
        ")\n",
        "\n",
        "# Loss function and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_1.parameters(), lr=0.001)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the correct device\n",
        "model_1.to(device)\n",
        "\n",
        "# Training utilities\n",
        "def train_step(model, data_loader, loss_fn, optimizer, accuracy_fn, device):\n",
        "    model.train()\n",
        "    train_loss, train_acc = 0, 0\n",
        "    for X, y in data_loader:\n",
        "        X, y = X.to(device), y.to(device)  # Move to device\n",
        "        y_pred = model(X)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "        train_acc += accuracy_fn(y, y_pred.argmax(dim=1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "def test_step(model, data_loader, loss_fn, accuracy_fn, device):\n",
        "    model.eval()\n",
        "    test_loss, test_acc = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_loader:\n",
        "            X, y = X.to(device), y.to(device)  # Move to device\n",
        "            y_pred = model(X)\n",
        "            test_loss += loss_fn(y_pred, y).item()\n",
        "            test_acc += accuracy_fn(y, y_pred.argmax(dim=1))\n",
        "    test_loss /= len(data_loader)\n",
        "    test_acc /= len(data_loader)\n",
        "    print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    return (y_true == y_pred).sum().item() / len(y_true) * 100\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "train_start_time_on_gpu = timer()\n",
        "\n",
        "# Set epochs\n",
        "epochs = 3\n",
        "\n",
        "# Training loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    train_step(model=model_1,\n",
        "               data_loader=train_dataloader,\n",
        "               loss_fn=loss_fn,\n",
        "               optimizer=optimizer,\n",
        "               accuracy_fn=accuracy_fn,\n",
        "               device=device)\n",
        "    test_step(model=model_1,\n",
        "              data_loader=test_dataloader,\n",
        "              loss_fn=loss_fn,\n",
        "              accuracy_fn=accuracy_fn,\n",
        "              device=device)\n",
        "\n",
        "train_end_time_on_gpu = timer()\n",
        "total_train_time_model_1 = train_end_time_on_gpu - train_start_time_on_gpu\n",
        "print(f\"Total training time: {total_train_time_model_1:.2f} seconds\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Bmy45eCywivy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "bva9aS7vzoct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_time_model_0, total_train_time_model_1"
      ],
      "metadata": {
        "id": "yJBYQdJ56nm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### It's interesting to see that the model ran on the GPU took about the same time to run as the one on the cpu! This is likely down to the fact that this model isn't that large, and our code for setting up the layers in the CNN is also not that complex.\n",
        "\n",
        "> **Note**: Sometimes, depending on your data/hardware you might find that your model trains faster on a CPU than a GPU\n",
        "\n",
        "> Why is this?\n",
        "> 1. It could be that the overheadfor copying data/model to and from the GPU outweighs the compute benefits offered by the GPU. So there is some extra time involved in copying the data to the GPU.\n",
        "> 2. The hardware you're using has a better CPU in terms of its capability than the GPU.\n",
        "\n",
        "See this article about gpu's:\n",
        "https://horace.io/brrr_intro.html"
      ],
      "metadata": {
        "id": "ijMHz2aYO3YN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now to evaluate our model we need to remember to put the results on the gpu!"
      ],
      "metadata": {
        "id": "PslYwQybTnev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, accuracy_fn, device=device):\n",
        "    '''\n",
        "    Returns a dictionary containing the results of model predicting on data_loader\n",
        "    '''\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in tqdm(data_loader):\n",
        "            # Make our data device agnostic\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            # Make predictions\n",
        "            y_pred = model(X) # note, don't have to specify model, see above\n",
        "\n",
        "            # Accumulate the loss and acc values per batch\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "        # Scale loss and acc to find the average loss/acc per batch\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n"
      ],
      "metadata": {
        "id": "7Jw4XOfATlIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_1 results dictionary\n",
        "model_1_results = eval_model(model=model_1,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn,\n",
        "                             device=device)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "Zf6ld4AgO13k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "compared with"
      ],
      "metadata": {
        "id": "YzhXgPNwYoXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_results"
      ],
      "metadata": {
        "id": "cH6RnIqvYLHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: Building Convolutional Neural Netowrk (CNN)\n",
        "\n",
        "CNN's arre known as ConvNET's\n",
        "\n",
        "CNN's are known for their capabilities to find patterns in visual data\n",
        "\n",
        "#### Below is a table outlining critical pieces of a CNN"
      ],
      "metadata": {
        "id": "CTlyzNRvZASR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Hyperparameter/Layer Type**         | **What does it do?**                                                                 | **Typical Values**                                                                                      |\n",
        "|----------------------------------------|-------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|\n",
        "| **Input image(s)**                     | Target images you'd like to discover patterns in                                     | Whatever you can take a photo (or video) of                                                            |\n",
        "| **Input layer**                        | Takes in target images and preprocesses them for further layers                     | `input_shape = [batch_size, image_height, image_width, color_channels]` (channels last) or              |\n",
        "|                                        |                                                                                     | `input_shape = [batch_size, color_channels, image_height, image_width]` (channels first)               |\n",
        "| **Convolution layer**                  | Extracts/learns the most important features from target images                      | Multiple, can create with `torch.nn.ConvXd()` (X can be multiple values)                               |\n",
        "| **Hidden activation/non-linear activation** | Adds non-linearity to learned features (non-straight lines)                         | Usually ReLU (`torch.nn.ReLU()`), though can be many more                                              |\n",
        "| **Pooling layer**                      | Reduces the dimensionality of learned image features                                | Max (`torch.nn.MaxPool2d()`) or Average (`torch.nn.AvgPool2d()`)                                       |\n",
        "| **Output layer/linear layer**          | Takes learned features and outputs them in shape of target labels                   | `torch.nn.Linear(out_features=[number_of_classes])` (e.g., 3 for pizza, steak, or sushi)               |\n",
        "| **Output activation**                  | Converts output logits to prediction probabilities                                  | `torch.sigmoid()` (binary classification) or `torch.softmax()` (multi-class classification)            |\n"
      ],
      "metadata": {
        "id": "OWRL_NVoadxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find out what's happening inside a CNN, see this website:\n",
        "\n",
        "https://poloclub.github.io/cnn-explainer/"
      ],
      "metadata": {
        "id": "cMCneqygijHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CNN\n",
        "class FashionMNISTModelV2(nn.Module):\n",
        "    '''\n",
        "    Model architecture that replicates that replicates the TinyVGG\n",
        "    model from the CNN Explainer website.\n",
        "    '''\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1), # values we can set ourselves- hyperparameters\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2)\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "            out_channels=hidden_units,\n",
        "            kernel_size=3,\n",
        "            padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "            out_channels=hidden_units,\n",
        "            kernel_size=3,\n",
        "            padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=hidden_units*7*7, # note what we are doing here to make the shapes match hidden units=10,shape =7x7 (see below)\n",
        "            out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block_1(x)\n",
        "        # print(f\"Output shape of conv_block_1: {x.shape}\")\n",
        "        x = self.conv_block_2(x)\n",
        "        # print(f\"Output shape of conv_block_2: {x.shape}\")\n",
        "        x = self.classifier(x)\n",
        "        # print(f\"Output shape of classifier: {x.shape}\")\n",
        "        return x"
      ],
      "metadata": {
        "id": "WKKwG535Yr_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_2 = FashionMNISTModelV2(input_shape=1, # since we're working with greyscale images the coloe channel is 1\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names)).to(device)"
      ],
      "metadata": {
        "id": "oeotTbRbedDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stepping through `nn.Conv2d()`\n",
        "```\n",
        "classtorch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)```\n",
        "\n",
        "docs:\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n"
      ],
      "metadata": {
        "id": "VvmW1-swHqWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_2.state_dict()"
      ],
      "metadata": {
        "id": "CthfgPVqKTdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a batch of images\n",
        "images = torch.randn(size=(32, 3, 64, 64))\n",
        "test_image = images[0]\n",
        "\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "print(f\"Single image shape: {test_image.shape}\")\n",
        "print(f\"Test image:\\n{test_image}\")"
      ],
      "metadata": {
        "id": "bDLEGTIXHI8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `nn.Conv2d` Calculations Explained\n",
        "\n",
        "The `nn.Conv2d` layer in PyTorch performs a 2D convolution operation, which is often used in image data processing. Below, we break down the key calculations involved:\n",
        "\n",
        "#### **Input Parameters**\n",
        "1. **Input Shape**: $(N, C_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$\n",
        "   - $N$: Batch size\n",
        "   - $C_{\\text{in}}$: Number of input channels\n",
        "   - $H_{\\text{in}}$: Height of the input\n",
        "   - $W_{\\text{in}}$: Width of the input\n",
        "\n",
        "2. **Kernel/Filter Size**: $(C_{\\text{out}}, C_{\\text{in}}, K_H, K_W)$\n",
        "   - $C_{\\text{out}}$: Number of output channels (filters)\n",
        "   - $K_H$: Kernel height\n",
        "   - $K_W$: Kernel width\n",
        "\n",
        "3. **Stride ($s$)**: Determines how much the filter shifts after each operation.\n",
        "4. **Padding ($p$)**: Adds zeros around the input to maintain or modify the output dimensions.\n",
        "5. **Dilation ($d$)**: Spacing between elements of the kernel.\n",
        "\n",
        "#### **Output Dimensions**\n",
        "The formula for the output height $(H_{\\text{out}})$ and output width $(W_{\\text{out}})$ is as follows:\n",
        "\n",
        "$$\n",
        "\\text{Output Dimension} = \\left\\lfloor \\frac{\\text{Input Dimension} + 2 \\times \\text{Padding} - \\text{Dilation} \\times (\\text{Kernel Size} - 1) - 1}{\\text{Stride}} + 1 \\right\\rfloor\n",
        "$$\n",
        "\n",
        "This can be rewritten explicitly for height and width:\n",
        "\n",
        "$$\n",
        "H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} + 2p - d(K_H - 1) - 1}{s} + 1 \\right\\rfloor\n",
        "$$\n",
        "\n",
        "$$\n",
        "W_{\\text{out}} = \\left\\lfloor \\frac{W_{\\text{in}} + 2p - d(K_W - 1) - 1}{s} + 1 \\right\\rfloor\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $p$ is the padding,\n",
        "- $d$ is the dilation,\n",
        "- $K_H$ and $K_W$ are the kernel dimensions,\n",
        "- $s$ is the stride.\n",
        "\n",
        "#### **Example**\n",
        "Suppose we have the following parameters:\n",
        "- Input shape: $(1, 3, 32, 32)$ (batch size of 1, 3 channels, 32x32 image)\n",
        "- Kernel size: $(16, 3, 5, 5)$ (16 filters, each 3x5x5)\n",
        "- Stride: $s = 1$\n",
        "- Padding: $p = 0$\n",
        "- Dilation: $d = 1$\n",
        "\n",
        "The output height and width are computed as:\n",
        "\n",
        "$$\n",
        "H_{\\text{out}} = \\left\\lfloor \\frac{32 + 2(0) - 1(5 - 1) - 1}{1} + 1 \\right\\rfloor = 28\n",
        "$$\n",
        "\n",
        "$$\n",
        "W_{\\text{out}} = \\left\\lfloor \\frac{32 + 2(0) - 1(5 - 1) - 1}{1} + 1 \\right\\rfloor = 28\n",
        "$$\n",
        "\n",
        "Thus, the output shape is $(1, 16, 28, 28)$.\n",
        "\n",
        "#### **Summary**\n",
        "The convolutional layer applies filters to the input, extracting spatial features using the parameters defined. Understanding these calculations is crucial for designing neural network architectures.\n"
      ],
      "metadata": {
        "id": "FFXgV2UeEMGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is the Derivation of the `nn.Conv2d` Equation?\n",
        "\n",
        "The `nn.Conv2d` layer performs a convolution operation by sliding a kernel/filter over the input image. Let's derive the formula for calculating the output dimensions.\n",
        "\n",
        "#### **Key Concepts**\n",
        "\n",
        "1. **Input Dimensions**: $(H_{\\text{in}}, W_{\\text{in}})$, where:\n",
        "   - $H_{\\text{in}}$: Height of the input image\n",
        "   - $W_{\\text{in}}$: Width of the input image\n",
        "\n",
        "2. **Kernel Dimensions**: $(K_H, K_W)$, where:\n",
        "   - $K_H$: Height of the kernel\n",
        "   - $K_W$: Width of the kernel\n",
        "\n",
        "3. **Stride ($s$)**: The number of pixels by which the kernel shifts after each operation.\n",
        "\n",
        "4. **Padding ($p$)**: Zeros added around the input to modify the effective size of the input.\n",
        "\n",
        "5. **Dilation ($d$)**: Spacing between kernel elements, which effectively enlarges the kernel size.\n",
        "\n",
        "#### **Steps to Derive the Formula**\n",
        "\n",
        "1. **Effective Input Size with Padding**  \n",
        "   Adding padding of size $p$ to both sides of the input increases the height and width of the input by $2p$ (padding is applied to both the top-bottom and left-right sides).  \n",
        "   The effective input dimensions become:\n",
        "\n",
        "   $$\n",
        "   H_{\\text{effective}} = H_{\\text{in}} + 2p\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   W_{\\text{effective}} = W_{\\text{in}} + 2p\n",
        "   $$\n",
        "\n",
        "2. **Effective Kernel Size with Dilation**  \n",
        "   When dilation is applied, the kernel size effectively increases by $(d - 1)$ spaces between elements. The effective kernel dimensions are:\n",
        "\n",
        "   $$\n",
        "   K_H^{\\text{effective}} = d \\cdot (K_H - 1) + 1\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   K_W^{\\text{effective}} = d \\cdot (K_W - 1) + 1\n",
        "   $$\n",
        "\n",
        "3. **Output Size Without Stride**  \n",
        "   If the kernel slides over the input without stride ($s = 1$), the output dimensions are the number of positions the kernel fits into. This is calculated as:\n",
        "\n",
        "   $$\n",
        "   H_{\\text{out}} = H_{\\text{effective}} - K_H^{\\text{effective}} + 1\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   W_{\\text{out}} = W_{\\text{effective}} - K_W^{\\text{effective}} + 1\n",
        "   $$\n",
        "\n",
        "4. **Incorporating Stride**  \n",
        "   Stride determines how far the kernel moves with each step. To account for stride, the output dimensions are divided by the stride value, and we take the floor to ensure integer results:\n",
        "\n",
        "   $$\n",
        "   H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{effective}} - K_H^{\\text{effective}} + 1}{s} \\right\\rfloor\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   W_{\\text{out}} = \\left\\lfloor \\frac{W_{\\text{effective}} - K_W^{\\text{effective}} + 1}{s} \\right\\rfloor\n",
        "   $$\n",
        "\n",
        "5. **Substituting Effective Dimensions**  \n",
        "   Substitute the values of $H_{\\text{effective}}$ and $K_H^{\\text{effective}}$:\n",
        "\n",
        "   $$\n",
        "   H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} + 2p - d \\cdot (K_H - 1) - 1}{s} + 1 \\right\\rfloor\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   W_{\\text{out}} = \\left\\lfloor \\frac{W_{\\text{in}} + 2p - d \\cdot (K_W - 1) - 1}{s} + 1 \\right\\rfloor\n",
        "   $$\n",
        "\n",
        "#### **Final Formula**\n",
        "\n",
        "The final formula for the output dimensions $(H_{\\text{out}}, W_{\\text{out}})$ is:\n",
        "\n",
        "$$\n",
        "H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} + 2p - d \\cdot (K_H - 1) - 1}{s} + 1 \\right\\rfloor\n",
        "$$\n",
        "\n",
        "$$\n",
        "W_{\\text{out}} = \\left\\lfloor \\frac{W_{\\text{in}} + 2p - d \\cdot (K_W - 1) - 1}{s} + 1 \\right\\rfloor\n",
        "$$\n",
        "\n",
        "#### **Sometimes these equations make more sense when we know where they come from. However, this is just my own curiosity**\n",
        "\n",
        "We see the formula takes into account the input dimensions, kernel size, stride, padding, and dilation to calculate the output dimensions of a 2D convolution operation. Understanding this derivation can (maybe) provide some clarity on how convolutional layers work in PyTorch.\n"
      ],
      "metadata": {
        "id": "izw0fiReGXD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding \"Number of Input Channels\" in CNNs\n",
        "\n",
        "In the context of Convolutional Neural Networks (CNNs), **input channels** refer to the number of distinct data \"layers\" or \"features\" present in the input tensor. This concept is crucial for understanding how CNNs process data, particularly when working with images.\n",
        "\n",
        "---\n",
        "\n",
        "## What is a Channel?\n",
        "\n",
        "A **channel** is a dimension of the input data that represents a specific type of information. The concept is most commonly associated with images, where each channel corresponds to a particular aspect of the image's color or intensity.\n",
        "\n",
        "### Example: RGB Images\n",
        "For a typical color image, the channels represent:\n",
        "- **Red (R)**\n",
        "- **Green (G)**\n",
        "- **Blue (B)**\n",
        "\n",
        "Thus, an RGB image has **3 channels**, and its dimensions are typically structured as:\n",
        "- `[Height, Width, Channels]` (e.g., `[256, 256, 3]` for a 256x256 color image).\n",
        "\n",
        "### Grayscale Images\n",
        "A grayscale image has only **1 channel** because it represents intensity values without any color information:\n",
        "- `[Height, Width, 1]` (e.g., `[256, 256, 1]` for a 256x256 grayscale image).\n",
        "\n",
        "---\n",
        "\n",
        "## Input Channels in CNNs\n",
        "\n",
        "When feeding data into a CNN, the **number of input channels** corresponds to the last dimension of the input tensor. For PyTorch and TensorFlow, the input tensor shapes are:\n",
        "- **PyTorch**: `[Batch Size, Channels, Height, Width]`\n",
        "- **TensorFlow**: `[Batch Size, Height, Width, Channels]`\n",
        "\n",
        "### Example\n",
        "- For an RGB image: 3 input channels.\n",
        "- For a grayscale image: 1 input channel.\n",
        "\n",
        "If you're working with data that has more features (e.g., hyperspectral imaging or multiple time-series features), the number of input channels will match the number of features.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Are Channels Important?\n",
        "\n",
        "1. **Filters Operate Per Channel**:\n",
        "   - Each filter in a CNN has a depth that matches the number of input channels.\n",
        "   - For example, a filter of size `[3x3]` for an RGB image will have dimensions `[3x3x3]`.\n",
        "\n",
        "2. **Feature Extraction**:\n",
        "   - Different channels can represent different features. For images, this could be colors (RGB), and for non-image data, it could represent any set of measurable quantities.\n",
        "\n",
        "3. **Scalability**:\n",
        "   - The network can handle inputs with multiple channels and extract meaningful features from all of them simultaneously.\n",
        "\n",
        "---\n",
        "\n",
        "## Channels Beyond Images\n",
        "\n",
        "Channels are not limited to images. In other types of data:\n",
        "- **Audio Data**: Channels could represent different frequency bands or stereo channels (left and right).\n",
        "- **Time-Series Data**: Channels could represent multiple variables measured over time (e.g., temperature, pressure, and humidity).\n",
        "- **Medical Imaging**: Channels could represent different imaging modalities (e.g., MRI, CT scans).\n",
        "\n",
        "---\n",
        "\n",
        "## Practical Notes\n",
        "\n",
        "1. **Mismatch in Channels**:\n",
        "   - The number of input channels must match the depth of the CNN's first layer. If they don't match, you'll get an error.\n",
        "   - Use layers like `torch.nn.Conv2d` (PyTorch) or `tf.keras.layers.Conv2D` (TensorFlow) to define filters that account for the number of input channels.\n",
        "\n",
        "2. **Expanding Channels**:\n",
        "   - For single-channel data like grayscale images, you may need to replicate or transform channels to match the input requirements of a pre-trained model (e.g., converting 1 channel to 3 channels for a model expecting RGB).\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "- A **channel** represents a distinct feature layer in the input data.\n",
        "- The **number of input channels** corresponds to the depth of the input tensor.\n",
        "- Channels are essential for feature extraction in CNNs and vary depending on the type of data being processed.\n"
      ],
      "metadata": {
        "id": "x3Y7QhpOKNZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a single conv2d\n",
        "torch.manual_seed(42)\n",
        "conv_layer = nn.Conv2d(in_channels=3,\n",
        "                       out_channels=10,\n",
        "                       kernel_size=(3,3), # kernel also known as a filter (3x3)\n",
        "                       stride=1,\n",
        "                       padding=1)\n",
        "\n",
        "# pass the data through the convolutional layer\n",
        "conv_output = conv_layer(test_image.unsqueeze(0))\n",
        "conv_output.shape"
      ],
      "metadata": {
        "id": "iXWH0cNkKIzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Pq2tzO9TRrNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stepping through the `nn.MaxPool2d` part\n",
        "```\n",
        "classtorch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
        "```\n",
        "docs:\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#maxpool2d"
      ],
      "metadata": {
        "id": "Rkttk1oMJRgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_image.shape"
      ],
      "metadata": {
        "id": "zTYFTFa8BVJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the Max Pooling Layer in CNNs\n",
        "\n",
        "When we pass data through a **Max Pooling Layer**, the goal is to reduce the spatial dimensions (Height and Width) of the input data while retaining the most important features. Below, we explain what happens when we apply a max pooling operation to an image after a convolutional layer.\n",
        "\n",
        "---\n",
        "\n",
        "## What is Max Pooling?\n",
        "\n",
        "**Max Pooling** is a down-sampling operation that:\n",
        "- Divides the input into smaller regions (defined by the pooling kernel size).\n",
        "- Outputs the maximum value from each region.\n",
        "\n",
        "This operation helps:\n",
        "1. **Reduce Computational Complexity**: By shrinking the spatial dimensions, max pooling reduces the number of parameters in the network.\n",
        "2. **Extract Dominant Features**: By keeping only the maximum values, it emphasizes the strongest activations in a region.\n",
        "3. **Improve Translational Invariance**: Small shifts in the input image won't drastically change the pooled output.\n",
        "\n",
        "---\n",
        "\n",
        "## Example: Max Pooling After a Convolutional Layer\n",
        "\n",
        "### Code Used in the Class\n",
        "\n",
        "```python\n",
        "# Create a single conv2d\n",
        "torch.manual_seed(42)\n",
        "conv_layer = nn.Conv2d(in_channels=3,\n",
        "                       out_channels=10,\n",
        "                       kernel_size=(3,3), # kernel also known as a filter (3x3)\n",
        "                       stride=1,\n",
        "                       padding=1)\n",
        "\n",
        "# Pass the data through the convolutional layer\n",
        "conv_output = conv_layer(test_image.unsqueeze(0))\n",
        "conv_output.shape\n"
      ],
      "metadata": {
        "id": "X6qkWASCWoi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the original image shape without unsqueeze\n",
        "print(f\"Test image original shape {test_image.shape}\")\n",
        "print(f\"Test image unsqueezed shape {test_image.unsqueeze(0).shape}\")\n",
        "\n",
        "# Create a sample nn.MaxPool2d layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2) # 2x2 square\n",
        "\n",
        "# Pass data through the conv layer only\n",
        "test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\n",
        "print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n",
        "\n",
        "# Pass data through the max pool layer- taking max of values\n",
        "test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\n",
        "print(f\"Shape after going through max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")\n"
      ],
      "metadata": {
        "id": "2_ueAKkHK08N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at this MaxPool function in a bit more detail"
      ],
      "metadata": {
        "id": "Y2fE6Af3ZWkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# Create a random tensor with a similar number of dimensions (4d)\n",
        "random_tensor = torch.rand(size=(1, 1, 2, 2))\n",
        "print(f\"\\nOur random tensor is:\\n {random_tensor}\")\n",
        "print(f\"Shape of our random tensor: {random_tensor.shape}\")\n",
        "\n",
        "# Create a max pool layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "# Pass the random tensor through the max pool layer\n",
        "max_pool_tensor = max_pool_layer(random_tensor)\n",
        "print(f\"\\nTensor after max pooling:\\n {max_pool_tensor}\")\n",
        "print(f\"Shape of tensor after max pooling: {max_pool_tensor.shape}\")\n"
      ],
      "metadata": {
        "id": "rB5HDcp9TuPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.squeeze(), cmap = \"magma\")"
      ],
      "metadata": {
        "id": "B-XQggs0Z2n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass image through model\n",
        "image.shape"
      ],
      "metadata": {
        "id": "wEERCNcNe97d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_image_tensor = torch.randn(1, 28, 28)\n",
        "rand_image_tensor.shape"
      ],
      "metadata": {
        "id": "vKlz31Anpr_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### you've got to remeber to put this on the device and we have accounted for the potential shape mismatch in the 7*7 in the forward pass above"
      ],
      "metadata": {
        "id": "ghpnhWenv-4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2(rand_image_tensor.unsqueeze(0).to(device))"
      ],
      "metadata": {
        "id": "EkWfksBNqU5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### e can see that everything is fine up until we get to our output layer, then we get a shape mismatch.\n",
        "\n",
        "our output from conv_block_2 to the output is flattened"
      ],
      "metadata": {
        "id": "YeGOcAcgtQLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up loss function and optimizer for model_2"
      ],
      "metadata": {
        "id": "oOf4elrgwjm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_2.parameters(),\n",
        "                            lr=0.1)"
      ],
      "metadata": {
        "id": "tkCiXZsjqwsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_model_2 = timer()\n",
        "\n",
        "# Set epochs\n",
        "epochs = 3\n",
        "\n",
        "# Create optimimization loop using train_step() and test_step()\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    train_step(model=model_2,\n",
        "               data_loader=train_dataloader,\n",
        "               loss_fn=loss_fn,\n",
        "               optimizer=optimizer,\n",
        "               accuracy_fn=accuracy_fn,\n",
        "               device=device)\n",
        "    test_step(model=model_2,\n",
        "              data_loader=test_dataloader,\n",
        "              loss_fn=loss_fn,\n",
        "              accuracy_fn=accuracy_fn,\n",
        "              device=device)\n",
        "\n",
        "train_time_end_model_2 = timer()\n",
        "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,      end=train_time_end_model_2, device=device)"
      ],
      "metadata": {
        "id": "u0omGqcuxoHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model_2_results = eval_model(model=model_2,\n",
        "                            data_loader=test_dataloader,\n",
        "                            loss_fn=loss_fn,\n",
        "                            accuracy_fn=accuracy_fn,\n",
        "                            device=device)\n",
        " model_2_results"
      ],
      "metadata": {
        "id": "sIUjfRZ80NPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare results across experiments"
      ],
      "metadata": {
        "id": "avS7WrxP11gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "compare_results = pd.DataFrame([model_0_results,\n",
        "                                model_1_results,\n",
        "                                model_2_results])\n",
        "compare_results"
      ],
      "metadata": {
        "id": "zOORh2LZ1Uvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add training time to results comparison\n",
        "compare_results[\"Training Time\"] = [f\"{total_train_time_model_0:.3f}\",\n",
        "                                     f\"{total_train_time_model_1:.3f}\",\n",
        "                                     f\"{total_train_time_model_2:.3f}\"]\n",
        "compare_results"
      ],
      "metadata": {
        "id": "5exBxUJW2mjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "plt.subplots(figsize=(8, 8))\n",
        "df_2dhist = pd.DataFrame({\n",
        "    x_label: grp['Training Time'].value_counts()\n",
        "    for x_label, grp in compare_results.groupby('model_name')\n",
        "})\n",
        "sns.heatmap(df_2dhist, cmap='magma')\n",
        "plt.xlabel('model_name')\n",
        "_ = plt.ylabel('Training Time')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-965zRAK4oef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Using dataframe compare_results: training time vs accuracy\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "# Create a scatter plot with training time on the x-axis and accuracy on the y-axis\n",
        "alt.Chart(compare_results).mark_circle().encode(\n",
        "    x='Training Time',\n",
        "    y='model_acc',\n",
        "    tooltip=['model_name', 'model_acc', 'Training Time']  # Add tooltips for more information\n",
        ").properties(\n",
        "    title='Training Time vs. Accuracy'  # Add a title to the chart\n",
        ")\n"
      ],
      "metadata": {
        "id": "xagvgqo54Kw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Model Results"
      ],
      "metadata": {
        "id": "s1HCZlJFfSKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\n",
        "plt.xlabel(\"accruracy (%)\")\n",
        "plt.ylabel(\"model\");"
      ],
      "metadata": {
        "id": "bWRyY9SF3l_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ultimately we want to viaulaize predictions\n",
        "**so let's use our best performing model to make predictions on random samples from the test_dataset**"
      ],
      "metadata": {
        "id": "D0w7BfbJhGmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n",
        "    pred_probs = []\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for sample in data:\n",
        "            # Prepare sample\n",
        "            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n",
        "\n",
        "            # Forward pass (model outputs raw logit)\n",
        "            pred_logit = model(sample)\n",
        "\n",
        "            # Get prediction probability (logit -> prediction probability)\n",
        "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 1, so can perform on dim=0)\n",
        "\n",
        "            # Get pred_prob off GPU for further calculations\n",
        "            pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "    # Stack the pred_probs to turn list into a tensor\n",
        "    return torch.stack(pred_probs)\n"
      ],
      "metadata": {
        "id": "pvR__Ud1giyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "test_samples = []\n",
        "test_labels = []\n",
        "for sample, label in random.sample(list(test_data), k=9):\n",
        "    test_samples.append(sample)\n",
        "    test_labels.append(label)\n",
        "\n",
        "# View the first test sample shape and label\n",
        "print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")"
      ],
      "metadata": {
        "id": "08jrSajUnmbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test samples with model 2\n",
        "pred_probs= make_predictions(model=model_2,\n",
        "                             data=test_samples)\n",
        "\n",
        "# View first two prediction probabilities list\n",
        "pred_probs[:2]"
      ],
      "metadata": {
        "id": "YLZ-rnxNfuf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test samples with model 2\n",
        "pred_probs= make_predictions(model=model_2,\n",
        "                             data=test_samples)\n",
        "\n",
        "# View first two prediction probabilities list\n",
        "pred_probs[:2]"
      ],
      "metadata": {
        "id": "iJu540cSoWeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Predictions\n",
        "pred_probs = make_predictions(model=model_2,\n",
        "                              data=test_samples,\n",
        "                              device=device)\n",
        "# View the first two prediction probabilities list\n",
        "pred_probs[:2]"
      ],
      "metadata": {
        "id": "bcSEivB-ohjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To get the test labels we will be using argmax()- see above"
      ],
      "metadata": {
        "id": "qvKn_NSPqRgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert prediction probabilities to labels\n",
        "# Turn the prediction probabilities into prediction labels by taking the argmax()\n",
        "pred_classes = pred_probs.argmax(dim=1)\n",
        "pred_classes"
      ],
      "metadata": {
        "id": "rf9FCecFp8PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**are they in the same format as our test labels?**\n",
        "✅"
      ],
      "metadata": {
        "id": "54-x5TsAq0Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Are our predictions in the same form as our test labels?\n",
        "test_labels, pred_classes"
      ],
      "metadata": {
        "id": "Bbr8g7OiqCyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Since we are making preditions on images let's plot those images along with the predictions"
      ],
      "metadata": {
        "id": "3ILz7taNrgc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions\n",
        "plt.figure(figsize=(9, 9))\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "for i, sample in enumerate(test_samples):\n",
        "  # Create a subplot\n",
        "  plt.subplot(nrows, ncols, i+1)\n",
        "\n",
        "  # Plot the target image\n",
        "  plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
        "\n",
        "  # Find the prediction label (in text form, e.g. \"Sandal\")\n",
        "  pred_label = class_names[pred_classes[i]]\n",
        "\n",
        "  # Get the truth label (in text form, e.g. \"T-shirt\")\n",
        "  truth_label = class_names[test_labels[i]]\n",
        "\n",
        "  # Create the title text of the plot\n",
        "  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
        "\n",
        "  # Check for equality and change title colour accordingly\n",
        "  if pred_label == truth_label:\n",
        "      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n",
        "  else:\n",
        "      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n",
        "  plt.axis(False);"
      ],
      "metadata": {
        "id": "nmdBYdYtq6_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So all of our pred_labels and truth_labels just happen to match up here, there are some in our dataset that don't match up"
      ],
      "metadata": {
        "id": "Y_zsWIrmv6JX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a confusion matrix to evaluate our results futher\n",
        "\n",
        "A confusion matrix is a great way to evaluate classsification models visually\n",
        "\n",
        "```\n",
        "classignite.metrics.confusion_matrix.ConfusionMatrix(num_classes, average=None, output_transform=<function ConfusionMatrix.<lambda>>, device=device(type='cpu'), skip_unrolling=True)\n",
        "```\n",
        "\n",
        "```\n",
        "metric = ConfusionMatrix(num_classes=3)\n",
        "metric.attach(default_evaluator, 'cm')\n",
        "y_true = torch.tensor([0, 1, 0, 1, 2])\n",
        "y_pred = torch.tensor([\n",
        "    [0.0, 1.0, 0.0],\n",
        "    [0.0, 1.0, 0.0],\n",
        "    [1.0, 0.0, 0.0],\n",
        "    [0.0, 1.0, 0.0],\n",
        "    [0.0, 1.0, 0.0],\n",
        "])\n",
        "state = default_evaluator.run([[y_pred, y_true]])\n",
        "print(state.metrics['cm'])\n",
        "```\n",
        "\n",
        "1. Make predictions with our trained model on our test datset\n",
        "2. Make a confusion matrix with `torch,etrics.ConfusionMatrix`\n",
        "3. Plot the confusion matrix using `mixtend.plotting.plot_confusion_matrix()`\n"
      ],
      "metadata": {
        "id": "G61RoNMMwqzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Make predictions with trained model\n",
        "y_preds = []\n",
        "model_2.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
        "    # Send data and targets to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # Do the forward pass\n",
        "    y_logit = model_2(X)\n",
        "    # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
        "    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)\n",
        "    # Put predictions on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "# Concatenate list of predictions into a tensor\n",
        "y_pred_tensor = torch.cat(y_preds)"
      ],
      "metadata": {
        "id": "pPwqmEOBr3LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_pred_tensor)"
      ],
      "metadata": {
        "id": "pbVbGMiX4Tc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install torchmetrics"
      ],
      "metadata": {
        "id": "WPmalRMv6eUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "rfdunRtw6V2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Torchmetrics does not seem to install with present torch load"
      ],
      "metadata": {
        "id": "eIdERULz9zTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mkxtend cersion: {mlxtend.__version__}\")\n",
        "    assert int(mlxtend.__version__.split(\".\") >=19, \"mlxtend version should be higher\")\n",
        "except:\n",
        "    !pip install torchmetrics -U mlxtend\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mkxtend version: {mlxtend.__version__}\")"
      ],
      "metadata": {
        "id": "b5FajaOX6k-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlxtend\n",
        "print(f\"mlxtend version: {mlxtend.__version__}\")"
      ],
      "metadata": {
        "id": "utAI2a-y6y7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <u>Remember</u>: *when you are working with confusion matrices and are working with matplotlib, you will have to turn the torch tesnsors back to NumPy format*"
      ],
      "metadata": {
        "id": "kL_YRpdQVStG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure y_pred_tensor contains predicted class indices\n",
        "if y_pred_tensor.ndim > 1:  # If y_pred_tensor contains logits or probabilities\n",
        "    y_pred_tensor = torch.argmax(y_pred_tensor, dim=1)\n",
        "\n",
        "# Check shape compatibility\n",
        "assert y_pred_tensor.shape == test_data.targets.shape, (\n",
        "    f\"Shape mismatch: preds={y_pred_tensor.shape}, targets={test_data.targets.shape}\"\n",
        ")\n",
        "\n",
        "# Initialize confusion matrix metric\n",
        "confmat = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_names))\n",
        "\n",
        "# Compute confusion matrix\n",
        "confmat_tensor = confmat(preds=y_pred_tensor, target=test_data.targets)\n",
        "\n",
        "# Display confusion matrix tensor\n",
        "print(confmat_tensor)\n",
        "\n",
        "# Plot confusion matrix\n",
        "confmat_array = confmat_tensor.numpy()\n",
        "fig, ax = plot_confusion_matrix(conf_mat=confmat_array,\n",
        "                                class_names=class_names,\n",
        "                                figsize=(10, 7))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "F_0SFw-RJyCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making confusion matrix look better with seaborn and cmap that highlights colors better and normalization- can be important for presentations to have colors with different values be more visible. Altering the cmap paramter can help you accomplish this."
      ],
      "metadata": {
        "id": "GeVxeLQWQOuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from torchmetrics import ConfusionMatrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure y_pred_tensor contains predicted class indices\n",
        "if y_pred_tensor.ndim > 1:  # If y_pred_tensor contains logits or probabilities\n",
        "    y_pred_tensor = torch.argmax(y_pred_tensor, dim=1)\n",
        "\n",
        "# Check shape compatibility\n",
        "assert y_pred_tensor.shape == test_data.targets.shape, (\n",
        "    f\"Shape mismatch: preds={y_pred_tensor.shape}, targets={test_data.targets.shape}\"\n",
        ")\n",
        "\n",
        "# Initialize confusion matrix metric\n",
        "confmat = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_names))\n",
        "\n",
        "# Compute confusion matrix\n",
        "confmat_tensor = confmat(preds=y_pred_tensor, target=test_data.targets)\n",
        "\n",
        "# Convert to NumPy array\n",
        "confmat_array = confmat_tensor.numpy()\n",
        "\n",
        "# Normalize the confusion matrix (optional)\n",
        "confmat_normalized = confmat_array / confmat_array.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confmat_normalized,\n",
        "            annot=True,             # Annotate with numbers\n",
        "            fmt=\".2f\",              # Format numbers to two decimal places\n",
        "            cmap=\"rocket_r\",           # Use a blue color map\n",
        "            xticklabels=class_names,  # Class names for x-axis\n",
        "            yticklabels=class_names,  # Class names for y-axis\n",
        "            cbar_kws={'label': 'Proportion'})  # Add color bar label\n",
        "\n",
        "# Add axis labels and a title\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Normalized Confusion Matrix\")\n",
        "plt.tight_layout()  # Adjust layout to prevent clipping\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5EwehN1NNSHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Just output- this is best for EDA applications, takes up smaller amount of memory and clearly states results for the EDA purpose."
      ],
      "metadata": {
        "id": "pBgluiOQSwgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confmat_tensor"
      ],
      "metadata": {
        "id": "9cKFS_m4QVM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does the confusion matrix tell us? It looks like our model is predicting high values of shirt, when the true label is T-shirt/top, the model is also predicting a shirt, when the true label is coat."
      ],
      "metadata": {
        "id": "kHCBqraOWYD_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4XP1YKBS8fa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}