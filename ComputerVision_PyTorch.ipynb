{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPlYodYlub9hIi+2cl6s7Lx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/PyTorch/blob/main/ComputerVision_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Vision- Using PYTorch\n",
        "\n",
        "**Basis**\n",
        "\n",
        "pixels are read as RGB colors and turned into --> numbers (tensors) or `numerical encoding` --> model (algorithm) --> output probability that the image is X ot Y or Z\n",
        "\n",
        "**Details**\n",
        " Tensors contain the following information:\n",
        " 1. Width of image\n",
        " 2. Height of image\n",
        " 3. Color channels == 3 (RGB)\n",
        " depending on what algorithm you're working with data as tensors whose ID is as follows:\n",
        "\n",
        " [batch_size, height, width, color_channels] OR [batch_size, color_channels, height, width]\n",
        "\n",
        " These will be mainly CNN models\n",
        "\n",
        " We will be working with `torch.nn.Conv2d`\n",
        "\n",
        " ## Computer version libraries in PyTorch\n",
        "\n",
        "* `torchvision`- base domain library for PyTorch computer vision-\n",
        "  https://pytorch.org/vision/stable/index.html\n",
        "* `torchvision.datassets`get datasets and loading functions here:\n",
        "  https://pytorch.org/vision/stable/datasets.html#built-in-datasets\n",
        "* `torchvision.models` get pre-trained computer vision models i.e. have pretrained weights, etc. that you can leverage for your own problems.\n",
        "* `torchvision.transforms`- functions for manipulating your vision data (images) to be suitable for use with an ML model.\n",
        "* `torch.utils.Dataset`- Base dataset class for PyTorch.\n",
        "* `torch.utils.data.DataLoader` - Creates a Python iterable over a dataset\n",
        "\n",
        "Torchvision supports common computer vision transformations in the torchvision.transforms and torchvision.transforms.v2 modules. Transforms can be used to transform or augment data for training or inference of different tasks (image classification, detection, segmentation, video classification).\n",
        "\n",
        "* PIL is the Python Imaging Library by Fredrik Lundh and contributors.\n",
        "\n",
        "### torchvision.datasets\n",
        "\n",
        "All datasets are subclasses of torch.utils.data.Dataset i.e, they have __getitem__ and __len__ methods implemented. Hence, they can all be passed to a torch.utils.data.DataLoader which can load multiple samples parallelly using torch.multiprocessing workers. For example:\n",
        "```\n",
        "imagenet_data = torchvision.datasets.ImageNet('path/to/imagenet_root/')\n",
        "data_loader = torch.utils.data.DataLoader(imagenet_data,\n",
        "                                          batch_size=4,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=args.nThreads)\n",
        "```"
      ],
      "metadata": {
        "id": "nPbopb9gp_qY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peQatlzGl9MS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting a dataset\n",
        "\n",
        "we will be using `fashion.mnist` datset- greyscale images of clothing\n",
        "basic dataset for implementation here\n",
        "\n",
        "Be aware that IMAGENET  is the gold standard for computer vision evaluations"
      ],
      "metadata": {
        "id": "vvnEhTotYExd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torchvision.datasets.FashionMNIST(root: str, train: bool = True, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) → None[source]`\n",
        "\n",
        "### Fashion-MNIST Dataset.\n",
        "\n",
        "Parameters:\n",
        "* **root (string)** – Root directory of dataset where FashionMNIST/processed/training.pt and FashionMNIST/processed/test.pt exist.\n",
        "* **train (bool, optional)** – If True, creates dataset from training.pt, otherwise from test.pt.\n",
        "* **download (bool, optional)** – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.\n",
        "transform (callable, optional) – A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop\n",
        "* **target_transform (callable, optional)** – A function/transform that takes in the target and transforms it."
      ],
      "metadata": {
        "id": "QKdOHeiRaYye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to\n",
        "    train=True, # do we want the training dataset?\n",
        "    download=True, # do we want to download?\n",
        "    transform=torchvision.transforms.ToTensor(), # how to transform the data\n",
        "    target_transform=None # how do we want to transform the labels/target\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        "    target_transform=None\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "RHElLWNrXinD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "l-ck3GS7c4T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the first training data- this will output the data as tensors (C x H x W) NOTE: grey scale images only have 1 color channel\n",
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "id": "lXZdKF5HdkNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "id": "Q4qCvtYvdzXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = train_data.class_to_idx\n",
        "class_to_idx"
      ],
      "metadata": {
        "id": "fJd2WzMseefo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets"
      ],
      "metadata": {
        "id": "WSzPIjWdekC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shape of our image\n",
        "print(f\"Image Shape: {image.shape} --> [color_channels, height, width], Image Label: {class_names[label]}\")"
      ],
      "metadata": {
        "id": "H6fZ_fyoeq2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing our data"
      ],
      "metadata": {
        "id": "uQfbCtS7lfZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "print(f\"Image Shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\") # had to remove a dimension so it would plot\n",
        "plt.title(class_names[label])\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(image.squeeze())\n",
        "# image"
      ],
      "metadata": {
        "id": "PwOKYwB6e0hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "row, cols = 4, 4\n",
        "for i in range(1, row * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    img, label = train_data[random_idx]\n",
        "    fig.add_subplot(row, cols, i)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    plt.title(class_names[label])\n",
        "    plt.axis(False)"
      ],
      "metadata": {
        "id": "hDfKBAFImDpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K2L4jBuJni32"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}